{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "These functions are here to install any necessary packages for the ML pipeline. Additionally, it mounts the Google drive for the access to the hosted data. Comment it out if it is unnecessary.\n",
        "\n",
        "None of this was generated using an LLM."
      ],
      "metadata": {
        "id": "izYMCbE4K10N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!pip install nilearn\n",
        "\n",
        "# Mount google drive for data folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x86z5LS-_ieB",
        "outputId": "618d4136-7b27-478e-e514-eb904b616b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.11.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.4.0)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nilearn) (24.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.15.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Downloading nilearn-0.11.1-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.11.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are all the imports for the various sections. They are collected here to provide a singular area to import them and make sure there are no duplicates. Claude Sonnet 3.7 frequently tried to import them multiple times."
      ],
      "metadata": {
        "id": "qWX3Bm9iXkVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import nibabel as nib\n",
        "from nilearn import plotting, image\n",
        "import pandas as pd\n",
        "from scipy.ndimage import zoom\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = 'drive/MyDrive/Colab Notebooks/ADNI'"
      ],
      "metadata": {
        "id": "jaCKc6DGWgJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is to gather the demographics of the patients and visualize how they are distributed. To keep things memory efficient, the MRI data was processed separately. Instead of using a larger file format that could incorporate more patient data, group, patient id, imaged id, age, and sex were used in that order to create the unique file names of the MRIs. So to collect demographic information, one must simply gather all the filenames and parse them.\n",
        "\n",
        "All of this code is hand written."
      ],
      "metadata": {
        "id": "2cXTgYnIDh05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.zip')]\n",
        "data_files = []\n",
        "\n",
        "for zip_file in zip_files:\n",
        "    with zipfile.ZipFile(f'{DATA_DIR}/{zip_file}', 'r') as zip:\n",
        "        for file in zip.infolist():\n",
        "            if file.filename.endswith('.npz'):\n",
        "                data_files.append(file.filename)"
      ],
      "metadata": {
        "id": "B1wDTdPaEMQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73X0e8rOqzaO",
        "outputId": "c9a30896-5f6f-4575-fd81-871ab4e73aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AD-099_S_0372-I49534-81-M.npz', 'AD-020_S_0213-I60608-63-M.npz', 'AD-133_S_1170-I149649-75-M.npz', 'AD-023_S_0084-I31201-75-F.npz', 'AD-126_S_0891-I124151-81-F.npz', 'AD-141_S_0340-I47861-84-F.npz', 'AD-130_S_1337-I80952-71-M.npz', 'AD-033_S_1281-I102011-80-F.npz', 'AD-057_S_1371-I62992-86-M.npz', 'AD-023_S_0916-I31532-80-M.npz', 'AD-033_S_0889-I51624-75-F.npz', 'AD-136_S_0426-I66777-81-M.npz', 'AD-024_S_1307-I77052-75-F.npz', 'AD-027_S_1082-I65364-71-F.npz', 'AD-022_S_0129-I59483-80-F.npz', 'AD-128_S_0216-I101997-85-M.npz', 'AD-033_S_0724-I54730-79-M.npz', 'AD-082_S_1377-I80387-84-M.npz', 'AD-029_S_1184-I67209-65-F.npz', 'AD-126_S_0784-I163717-78-F.npz', 'AD-057_S_1373-I80257-76-M.npz', 'AD-029_S_0999-I64893-71-M.npz', 'AD-127_S_1382-I66309-65-M.npz', 'AD-114_S_0374-I39816-76-F.npz', 'AD-116_S_0487-I89661-78-M.npz', 'AD-128_S_0740-I69059-73-M.npz', 'AD-023_S_0916-I89742-80-M.npz', 'AD-114_S_0228-I49734-80-F.npz', 'AD-057_S_1371-I109121-86-M.npz', 'AD-014_S_0328-I70656-77-M.npz', 'AD-022_S_0129-I59474-80-F.npz', 'AD-128_S_0266-I68914-88-M.npz', 'AD-094_S_1402-I85602-70-M.npz', 'AD-014_S_0328-I39619-77-M.npz', 'AD-133_S_1170-I92078-75-M.npz', 'AD-005_S_1341-I60416-72-F.npz', 'AD-029_S_0836-I79586-84-M.npz', 'AD-033_S_0724-I123023-79-M.npz', 'AD-022_S_0543-I59542-74-F.npz', 'AD-032_S_1101-I90970-72-F.npz', 'AD-027_S_1081-I73936-86-M.npz', 'AD-130_S_0956-I82690-65-F.npz', 'AD-141_S_1137-I106231-83-F.npz', 'AD-126_S_0606-I72872-69-F.npz', 'AD-002_S_0619-I120963-80-M.npz', 'AD-141_S_0696-I81485-74-F.npz', 'AD-036_S_1001-I71575-69-M.npz', 'AD-027_S_0404-I34203-89-F.npz', 'AD-109_S_1192-I63497-70-F.npz', 'AD-126_S_0891-I39040-81-F.npz', 'AD-005_S_0814-I73528-72-F.npz', 'AD-131_S_0457-I72918-84-F.npz', 'AD-099_S_0372-I34544-81-M.npz', 'AD-130_S_1290-I79043-80-F.npz', 'AD-131_S_0497-I123415-76-M.npz', 'AD-114_S_0979-I132432-86-M.npz', 'AD-126_S_0606-I48922-69-F.npz', 'AD-067_S_0029-I34769-65-M.npz', 'AD-029_S_0999-I101574-71-M.npz', 'AD-033_S_1285-I101709-81-F.npz', 'AD-007_S_1339-I78753-80-F.npz', 'AD-067_S_1253-I80302-63-F.npz', 'AD-094_S_1090-I91033-72-M.npz', 'AD-067_S_0812-I138848-74-F.npz', 'AD-067_S_0812-I38725-74-F.npz', 'AD-057_S_1379-I80266-88-M.npz', 'AD-099_S_0470-I88562-88-F.npz', 'AD-033_S_1285-I139245-81-F.npz', 'AD-007_S_1304-I59909-75-F.npz', 'AD-128_S_0167-I97235-78-M.npz', 'AD-073_S_0565-I75475-76-M.npz', 'AD-062_S_0730-I85958-73-F.npz', 'AD-127_S_0844-I129619-86-F.npz', 'AD-114_S_0374-I49747-76-F.npz', 'AD-067_S_0076-I34787-78-M.npz', 'AD-021_S_0753-I68480-66-M.npz', 'AD-029_S_1056-I71490-71-F.npz', 'AD-099_S_0372-I108066-81-M.npz', 'AD-027_S_1082-I86376-71-F.npz', 'AD-094_S_1397-I124920-56-F.npz', 'AD-023_S_0084-I31160-75-F.npz', 'AD-067_S_0020-I66035-72-F.npz', 'AD-033_S_1283-I54784-60-M.npz', 'AD-014_S_0357-I39637-71-F.npz', 'AD-011_S_0010-I32269-74-F.npz', 'AD-033_S_1308-I102377-80-M.npz', 'AD-067_S_1185-I79231-63-M.npz', 'AD-141_S_0852-I94925-86-F.npz', 'AD-041_S_1391-I62932-85-M.npz', 'AD-136_S_0300-I81600-58-M.npz', 'AD-014_S_0356-I39628-81-M.npz', 'AD-023_S_0139-I52057-67-F.npz', 'AD-011_S_0183-I32009-75-F.npz', 'AD-067_S_0076-I35910-78-M.npz', 'AD-027_S_1081-I47167-86-M.npz', 'AD-023_S_0916-I124096-80-M.npz', 'AD-131_S_0457-I112365-84-F.npz', 'AD-023_S_1262-I143684-75-F.npz', 'AD-073_S_1207-I63126-57-F.npz', 'AD-016_S_0991-I40789-87-F.npz', 'CN-007_S_1222-I59985-76-F.npz', 'CN-057_S_0779-I80211-83-M.npz', 'CN-002_S_1261-I139509-71-F.npz', 'CN-029_S_0843-I88412-72-M.npz', 'CN-062_S_0768-I80275-79-M.npz', 'CN-023_S_0058-I171379-71-M.npz', 'CN-041_S_1002-I139310-76-F.npz', 'CN-114_S_0166-I148899-74-F.npz', 'CN-136_S_0184-I83848-78-F.npz', 'CN-033_S_1016-I86244-82-F.npz', 'CN-036_S_1023-I139411-80-F.npz', 'CN-098_S_0896-I162827-77-M.npz', 'CN-037_S_0303-I138805-88-M.npz', 'CN-131_S_0123-I70875-75-M.npz', 'CN-033_S_0734-I217905-74-M.npz', 'CN-131_S_1301-I63792-74-F.npz', 'CN-036_S_0576-I68512-79-M.npz', 'CN-126_S_0605-I38873-76-F.npz', 'CN-005_S_0602-I162038-73-M.npz', 'CN-018_S_0043-I171150-81-M.npz', 'CN-116_S_1249-I172373-74-F.npz', 'CN-005_S_0223-I100740-78-F.npz', 'CN-007_S_0068-I176199-80-F.npz', 'CN-126_S_0405-I48885-77-M.npz', 'CN-130_S_0232-I65488-79-M.npz', 'CN-099_S_0090-I92332-70-M.npz', 'CN-023_S_0081-I52021-74-M.npz', 'CN-099_S_0352-I67741-77-F.npz', 'CN-027_S_0074-I34104-82-M.npz', 'CN-041_S_0898-I162791-86-F.npz', 'CN-027_S_0118-I137254-81-M.npz', 'CN-007_S_1206-I96023-76-M.npz', 'CN-035_S_0048-I83502-80-M.npz', 'CN-023_S_0926-I162318-72-F.npz', 'CN-141_S_0767-I47301-77-F.npz', 'CN-033_S_1086-I82322-82-M.npz', 'CN-014_S_0520-I123634-79-F.npz', 'CN-136_S_0186-I40215-83-F.npz', 'CN-023_S_0963-I65003-74-M.npz', 'CN-067_S_0059-I38695-72-F.npz', 'CN-941_S_1203-I63878-84-M.npz', 'CN-027_S_0403-I47060-76-M.npz', 'CN-011_S_0016-I124730-66-M.npz', 'CN-002_S_1280-I138889-71-F.npz', 'CN-011_S_0002-I35474-74-M.npz', 'CN-094_S_0526-I80708-86-F.npz', 'CN-005_S_0223-I32856-78-F.npz', 'CN-099_S_0352-I34536-77-F.npz', 'CN-005_S_0553-I71311-88-M.npz', 'CN-024_S_1063-I63387-80-F.npz', 'CN-023_S_0963-I162345-74-M.npz', 'CN-098_S_0172-I65756-74-F.npz', 'CN-051_S_1123-I142097-76-F.npz', 'CN-127_S_0622-I120579-78-F.npz', 'CN-006_S_0681-I124681-77-F.npz', 'CN-020_S_0899-I124821-81-F.npz', 'CN-022_S_0066-I59422-76-M.npz', 'CN-011_S_0021-I200398-73-F.npz', 'CN-029_S_0866-I65610-81-F.npz', 'CN-029_S_0843-I66996-72-M.npz', 'CN-099_S_0534-I112318-66-M.npz', 'CN-011_S_0022-I32373-63-M.npz', 'CN-023_S_1190-I73337-78-F.npz', 'CN-041_S_0125-I39467-75-M.npz', 'CN-011_S_0005-I123984-77-M.npz', 'CN-141_S_0726-I94823-81-M.npz', 'CN-062_S_0768-I50514-79-M.npz', 'CN-051_S_1123-I99261-76-F.npz', 'CN-057_S_0779-I166938-83-M.npz', 'CN-006_S_0484-I65565-72-M.npz', 'CN-027_S_0074-I172001-82-M.npz', 'CN-094_S_0711-I123252-80-M.npz', 'CN-131_S_0436-I48019-85-M.npz', 'CN-002_S_1280-I81320-71-F.npz', 'CN-032_S_0677-I90950-72-M.npz', 'CN-016_S_0538-I78822-83-M.npz', 'CN-126_S_0680-I38921-80-M.npz', 'CN-014_S_0519-I39646-77-M.npz', 'CN-941_S_1197-I108335-85-F.npz', 'CN-002_S_1280-I60055-71-F.npz', 'CN-128_S_0500-I68959-79-M.npz', 'CN-022_S_0014-I87009-81-F.npz', 'CN-020_S_0883-I200228-78-F.npz', 'CN-023_S_0058-I132349-71-M.npz', 'CN-057_S_0818-I166947-76-F.npz', 'CN-002_S_0295-I123684-88-M.npz', 'CN-027_S_0118-I47148-81-M.npz', 'CN-130_S_0886-I80943-71-F.npz', 'CN-136_S_0196-I63827-81-F.npz', 'CN-020_S_1288-I78859-61-M.npz', 'CN-094_S_1241-I65413-75-M.npz', 'CN-022_S_0014-I59374-81-F.npz', 'CN-130_S_1200-I204423-85-M.npz', 'CN-127_S_0259-I75331-72-M.npz', 'CN-014_S_0520-I218435-79-F.npz', 'CN-062_S_0578-I111318-78-F.npz', 'CN-023_S_0031-I163031-78-F.npz', 'CN-002_S_0559-I171077-79-M.npz', 'CN-022_S_0096-I96247-83-M.npz', 'CN-041_S_0262-I204768-90-M.npz', 'MCI-128_S_0200-I98865-63-M.npz', 'MCI-041_S_1260-I142067-73-M.npz', 'MCI-073_S_0909-I167020-71-F.npz', 'MCI-011_S_0326-I67357-79-M.npz', 'MCI-141_S_1004-I85687-75-F.npz', 'MCI-094_S_1314-I63195-84-F.npz', 'MCI-023_S_0604-I87072-90-M.npz', 'MCI-023_S_0855-I64817-76-M.npz', 'MCI-098_S_0269-I102795-67-M.npz', 'MCI-006_S_0675-I62569-81-F.npz', 'MCI-033_S_0513-I42266-72-M.npz', 'MCI-011_S_0362-I89400-71-F.npz', 'MCI-053_S_0507-I75461-57-F.npz', 'MCI-002_S_1070-I121071-75-M.npz', 'MCI-136_S_1227-I134214-66-F.npz', 'MCI-082_S_0928-I39800-84-F.npz', 'MCI-126_S_0865-I85631-75-M.npz', 'MCI-073_S_0746-I176881-75-F.npz', 'MCI-029_S_1218-I172267-87-F.npz', 'MCI-023_S_1046-I46391-73-M.npz', 'MCI-127_S_0925-I129641-76-M.npz', 'MCI-053_S_0389-I107984-73-F.npz', 'MCI-130_S_0285-I80916-67-M.npz', 'MCI-067_S_0045-I35883-87-M.npz', 'MCI-130_S_0289-I63715-68-M.npz', 'MCI-016_S_1092-I76591-77-M.npz', 'MCI-062_S_1182-I123234-78-F.npz', 'MCI-036_S_0656-I71535-84-M.npz', 'MCI-023_S_0376-I149779-71-M.npz', 'MCI-027_S_0644-I68087-78-M.npz', 'MCI-051_S_1131-I91823-88-M.npz', 'MCI-013_S_0240-I63369-90-M.npz', 'MCI-023_S_0042-I132329-77-M.npz', 'MCI-027_S_1387-I121784-87-M.npz', 'MCI-035_S_0033-I45165-87-M.npz', 'MCI-029_S_1318-I149561-85-F.npz', 'MCI-027_S_1045-I47208-84-F.npz', 'MCI-027_S_0485-I37556-66-M.npz', 'MCI-011_S_0168-I35490-90-M.npz', 'MCI-005_S_0324-I83333-79-F.npz', 'MCI-007_S_0414-I64781-81-F.npz', 'MCI-131_S_1389-I81971-78-M.npz', 'MCI-027_S_0644-I34239-78-M.npz', 'MCI-116_S_0649-I94630-88-M.npz', 'MCI-011_S_0861-I35512-88-M.npz', 'MCI-027_S_0116-I172010-81-M.npz', 'MCI-013_S_0240-I70647-90-M.npz', 'MCI-023_S_0376-I31382-71-M.npz', 'MCI-037_S_0501-I79779-78-M.npz', 'MCI-130_S_0285-I39118-67-M.npz', 'MCI-011_S_1282-I62635-77-F.npz', 'MCI-027_S_0307-I149534-81-M.npz', 'MCI-114_S_0378-I99103-70-F.npz', 'MCI-035_S_0292-I39582-80-M.npz', 'MCI-126_S_1187-I122359-78-M.npz', 'MCI-067_S_0336-I64313-77-M.npz', 'MCI-033_S_0513-I151037-72-M.npz', 'MCI-051_S_1331-I120543-78-F.npz', 'MCI-094_S_1314-I160058-84-F.npz', 'MCI-141_S_0697-I91230-89-M.npz', 'MCI-005_S_0572-I162020-81-M.npz', 'MCI-023_S_0217-I143347-87-F.npz', 'MCI-007_S_0128-I59862-65-F.npz', 'MCI-007_S_0344-I62607-81-M.npz', 'MCI-027_S_1045-I162385-84-F.npz', 'MCI-052_S_1352-I104379-87-M.npz', 'MCI-014_S_0557-I71420-74-M.npz', 'MCI-011_S_0241-I143461-86-M.npz', 'MCI-109_S_1343-I95678-65-M.npz', 'MCI-023_S_1046-I109496-73-M.npz', 'MCI-128_S_0227-I103493-82-F.npz', 'MCI-016_S_1121-I96238-58-F.npz', 'MCI-141_S_0915-I48562-80-F.npz', 'MCI-141_S_1245-I106629-72-F.npz', 'MCI-136_S_1227-I79070-66-F.npz', 'MCI-033_S_0513-I42257-72-M.npz', 'MCI-036_S_0673-I91689-81-M.npz', 'MCI-016_S_1121-I132632-58-F.npz', 'MCI-023_S_0376-I52088-71-M.npz', 'MCI-035_S_0033-I87587-87-M.npz', 'MCI-014_S_0169-I76582-74-F.npz', 'MCI-126_S_0865-I112336-75-M.npz', 'MCI-130_S_0505-I70866-81-M.npz', 'MCI-003_S_0908-I64561-63-F.npz', 'MCI-023_S_0126-I171406-79-F.npz', 'MCI-027_S_0307-I176300-81-M.npz', 'MCI-027_S_0644-I47099-78-M.npz', 'MCI-062_S_1299-I204098-73-M.npz', 'MCI-031_S_0568-I111223-80-F.npz', 'MCI-022_S_1097-I112211-73-M.npz', 'MCI-036_S_0673-I123100-81-M.npz', 'MCI-141_S_0982-I91293-81-F.npz', 'MCI-130_S_0102-I39148-71-F.npz', 'MCI-033_S_1309-I78959-70-F.npz', 'MCI-057_S_0464-I85566-85-M.npz', 'MCI-126_S_0865-I79380-75-M.npz', 'MCI-022_S_0924-I88509-70-M.npz', 'MCI-027_S_0835-I102716-75-M.npz', 'MCI-051_S_1072-I94549-60-F.npz', 'MCI-023_S_0030-I129259-82-F.npz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demographics = []\n",
        "\n",
        "for file in data_files:\n",
        "    splits = file.split('-')\n",
        "    demographics.append({'age': int(splits[3]), 'sex': splits[4].split('.')[0]})\n",
        "\n",
        "print(demographics)"
      ],
      "metadata": {
        "id": "5G21H6CkrQGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868168b1-b069-41ae-975e-880e06555007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'age': 81, 'sex': 'M'}, {'age': 63, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 75, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 71, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 86, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 75, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 75, 'sex': 'F'}, {'age': 71, 'sex': 'F'}, {'age': 80, 'sex': 'F'}, {'age': 85, 'sex': 'M'}, {'age': 79, 'sex': 'M'}, {'age': 84, 'sex': 'M'}, {'age': 65, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 76, 'sex': 'M'}, {'age': 71, 'sex': 'M'}, {'age': 65, 'sex': 'M'}, {'age': 76, 'sex': 'F'}, {'age': 78, 'sex': 'M'}, {'age': 73, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 86, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 88, 'sex': 'M'}, {'age': 70, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 72, 'sex': 'F'}, {'age': 84, 'sex': 'M'}, {'age': 79, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 72, 'sex': 'F'}, {'age': 86, 'sex': 'M'}, {'age': 65, 'sex': 'F'}, {'age': 83, 'sex': 'F'}, {'age': 69, 'sex': 'F'}, {'age': 80, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 69, 'sex': 'M'}, {'age': 89, 'sex': 'F'}, {'age': 70, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 72, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 76, 'sex': 'M'}, {'age': 86, 'sex': 'M'}, {'age': 69, 'sex': 'F'}, {'age': 65, 'sex': 'M'}, {'age': 71, 'sex': 'M'}, {'age': 81, 'sex': 'F'}, {'age': 80, 'sex': 'F'}, {'age': 63, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 74, 'sex': 'F'}, {'age': 88, 'sex': 'M'}, {'age': 88, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 75, 'sex': 'F'}, {'age': 78, 'sex': 'M'}, {'age': 76, 'sex': 'M'}, {'age': 73, 'sex': 'F'}, {'age': 86, 'sex': 'F'}, {'age': 76, 'sex': 'F'}, {'age': 78, 'sex': 'M'}, {'age': 66, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 56, 'sex': 'F'}, {'age': 75, 'sex': 'F'}, {'age': 72, 'sex': 'F'}, {'age': 60, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 74, 'sex': 'F'}, {'age': 80, 'sex': 'M'}, {'age': 63, 'sex': 'M'}, {'age': 86, 'sex': 'F'}, {'age': 85, 'sex': 'M'}, {'age': 58, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 67, 'sex': 'F'}, {'age': 75, 'sex': 'F'}, {'age': 78, 'sex': 'M'}, {'age': 86, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 84, 'sex': 'F'}, {'age': 75, 'sex': 'F'}, {'age': 57, 'sex': 'F'}, {'age': 87, 'sex': 'F'}, {'age': 76, 'sex': 'F'}, {'age': 83, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 79, 'sex': 'M'}, {'age': 71, 'sex': 'M'}, {'age': 76, 'sex': 'F'}, {'age': 74, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 82, 'sex': 'F'}, {'age': 80, 'sex': 'F'}, {'age': 77, 'sex': 'M'}, {'age': 88, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 74, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 79, 'sex': 'M'}, {'age': 76, 'sex': 'F'}, {'age': 73, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 80, 'sex': 'F'}, {'age': 77, 'sex': 'M'}, {'age': 79, 'sex': 'M'}, {'age': 70, 'sex': 'M'}, {'age': 74, 'sex': 'M'}, {'age': 77, 'sex': 'F'}, {'age': 82, 'sex': 'M'}, {'age': 86, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 76, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 72, 'sex': 'F'}, {'age': 77, 'sex': 'F'}, {'age': 82, 'sex': 'M'}, {'age': 79, 'sex': 'F'}, {'age': 83, 'sex': 'F'}, {'age': 74, 'sex': 'M'}, {'age': 72, 'sex': 'F'}, {'age': 84, 'sex': 'M'}, {'age': 76, 'sex': 'M'}, {'age': 66, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 74, 'sex': 'M'}, {'age': 86, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 77, 'sex': 'F'}, {'age': 88, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 74, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 76, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 77, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 76, 'sex': 'M'}, {'age': 73, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 66, 'sex': 'M'}, {'age': 63, 'sex': 'M'}, {'age': 78, 'sex': 'F'}, {'age': 75, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 79, 'sex': 'M'}, {'age': 76, 'sex': 'F'}, {'age': 83, 'sex': 'M'}, {'age': 72, 'sex': 'M'}, {'age': 82, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 85, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 83, 'sex': 'M'}, {'age': 80, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 85, 'sex': 'F'}, {'age': 71, 'sex': 'F'}, {'age': 79, 'sex': 'M'}, {'age': 81, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 71, 'sex': 'M'}, {'age': 76, 'sex': 'F'}, {'age': 88, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 61, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 81, 'sex': 'F'}, {'age': 85, 'sex': 'M'}, {'age': 72, 'sex': 'M'}, {'age': 79, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 78, 'sex': 'F'}, {'age': 79, 'sex': 'M'}, {'age': 83, 'sex': 'M'}, {'age': 90, 'sex': 'M'}, {'age': 63, 'sex': 'M'}, {'age': 73, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 79, 'sex': 'M'}, {'age': 75, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 90, 'sex': 'M'}, {'age': 76, 'sex': 'M'}, {'age': 67, 'sex': 'M'}, {'age': 81, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 71, 'sex': 'F'}, {'age': 57, 'sex': 'F'}, {'age': 75, 'sex': 'M'}, {'age': 66, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 75, 'sex': 'M'}, {'age': 75, 'sex': 'F'}, {'age': 87, 'sex': 'F'}, {'age': 73, 'sex': 'M'}, {'age': 76, 'sex': 'M'}, {'age': 73, 'sex': 'F'}, {'age': 67, 'sex': 'M'}, {'age': 87, 'sex': 'M'}, {'age': 68, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 78, 'sex': 'F'}, {'age': 84, 'sex': 'M'}, {'age': 71, 'sex': 'M'}, {'age': 78, 'sex': 'M'}, {'age': 88, 'sex': 'M'}, {'age': 90, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 87, 'sex': 'M'}, {'age': 87, 'sex': 'M'}, {'age': 85, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 66, 'sex': 'M'}, {'age': 90, 'sex': 'M'}, {'age': 79, 'sex': 'F'}, {'age': 81, 'sex': 'F'}, {'age': 78, 'sex': 'M'}, {'age': 78, 'sex': 'M'}, {'age': 88, 'sex': 'M'}, {'age': 88, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 90, 'sex': 'M'}, {'age': 71, 'sex': 'M'}, {'age': 78, 'sex': 'M'}, {'age': 67, 'sex': 'M'}, {'age': 77, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 70, 'sex': 'F'}, {'age': 80, 'sex': 'M'}, {'age': 78, 'sex': 'M'}, {'age': 77, 'sex': 'M'}, {'age': 72, 'sex': 'M'}, {'age': 78, 'sex': 'F'}, {'age': 84, 'sex': 'F'}, {'age': 89, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 87, 'sex': 'F'}, {'age': 65, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 84, 'sex': 'F'}, {'age': 87, 'sex': 'M'}, {'age': 74, 'sex': 'M'}, {'age': 86, 'sex': 'M'}, {'age': 65, 'sex': 'M'}, {'age': 73, 'sex': 'M'}, {'age': 82, 'sex': 'F'}, {'age': 58, 'sex': 'F'}, {'age': 80, 'sex': 'F'}, {'age': 72, 'sex': 'F'}, {'age': 66, 'sex': 'F'}, {'age': 72, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 58, 'sex': 'F'}, {'age': 71, 'sex': 'M'}, {'age': 87, 'sex': 'M'}, {'age': 74, 'sex': 'F'}, {'age': 75, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 63, 'sex': 'F'}, {'age': 79, 'sex': 'F'}, {'age': 81, 'sex': 'M'}, {'age': 78, 'sex': 'M'}, {'age': 73, 'sex': 'M'}, {'age': 80, 'sex': 'F'}, {'age': 73, 'sex': 'M'}, {'age': 81, 'sex': 'M'}, {'age': 81, 'sex': 'F'}, {'age': 71, 'sex': 'F'}, {'age': 70, 'sex': 'F'}, {'age': 85, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 70, 'sex': 'M'}, {'age': 75, 'sex': 'M'}, {'age': 60, 'sex': 'F'}, {'age': 82, 'sex': 'F'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_demos = pd.DataFrame(demographics)\n",
        "df_demos.plot(kind='hist', grid=False, rwidth=0.8, title='All Ages')\n",
        "plt.show()\n",
        "\n",
        "df_demos.agg({'age': ['min', 'max', 'mean']})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "qtcW2RAD0yUY",
        "outputId": "2c288796-8e61-4468-ccb1-0f1bbd08fec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age\n",
              "min   56.0\n",
              "max   90.0\n",
              "mean  76.8"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5758020-c587-4847-8191-ce0cfb39e709\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>56.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>76.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5758020-c587-4847-8191-ce0cfb39e709')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5758020-c587-4847-8191-ce0cfb39e709 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5758020-c587-4847-8191-ce0cfb39e709');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8479fc2c-ef7d-4cc2-ba61-bbcc3644ecef\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8479fc2c-ef7d-4cc2-ba61-bbcc3644ecef')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8479fc2c-ef7d-4cc2-ba61-bbcc3644ecef button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_demos\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.140984024650784,\n        \"min\": 56.0,\n        \"max\": 90.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          56.0,\n          90.0,\n          76.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_demos['sex'].value_counts().plot(kind='bar', title='Sexes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ye61Qf0J4slk",
        "outputId": "f7d61880-c363-4bf4-f3a8-a283d81dfea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Sexes'}, xlabel='sex'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "males = df_demos[df_demos['sex'] == 'M']\n",
        "females = df_demos[df_demos['sex'] == 'F']\n",
        "\n",
        "x = males['age']\n",
        "y = females['age']\n",
        "\n",
        "plt.hist(x, alpha=0.5, label='males')\n",
        "plt.hist(y, alpha=0.5, label='females')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Male/Female')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ypAnlEFG2WLM",
        "outputId": "30b7a147-5561-45d5-d169-e6769dce72b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGzCAYAAACPa3XZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMEtJREFUeJzt3Xl0FFXexvGnCdAJZJOQFUJI2JFFFgfCLnsGGAUUBdSgDm7IKyCi0QEFlKCOC74iLqMwLizCDKggMIACLxhQcBBXIBAmIDuSBBJJILnvHxx6bBKWQOd2Er6fc+qc7ntvVf360lYeq6urHcYYIwAAAEsqeLsAAABwdSF8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifABQ7dq1NWzYMG+X4VW7d++Ww+HQrFmzvF0KUO4RPoAyYNasWXI4HHI4HFq3bl2hfmOMoqOj5XA41LdvXy9UKD3yyCNq3LixJPd6z10ef/xxr9QHoPSo6O0CAFw6X19fzZ49Wx06dHBrX7Nmjfbu3Sun0+mlyqQlS5aoX79+bm2TJk1SbGysW1uTJk1slgWgFCJ8AGXIH//4R82fP1+vvvqqKlb873++s2fPVqtWrXTkyBGv1LVr1y5t27ZNb7zxhlt7QkKCWrdu7ZWaAJRefOwClCGDBw/W0aNHtWLFCldbXl6eFixYoCFDhhQa/9e//lXt2rVTSEiI/Pz81KpVKy1YsOCS9pWRkaFRo0YpOjpaTqdTdevW1XPPPaeCgoJCY5csWaKgoKBCZ2QuZunSperYsaOqVq2qgIAA9enTRz/88IPbmGHDhsnf31/p6enq27ev/P39VaNGDU2fPl2S9N1336lr166qWrWqYmJiNHv2bLf1f/31V40dO1ZNmzaVv7+/AgMDlZCQoG+//faSavz555918803q1q1avL19VXr1q31ySefFOt1AnBH+ADKkNq1ays+Pl5z5sxxtS1dulSZmZm67bbbCo2fNm2aWrRooUmTJmnKlCmqWLGibrnlFi1ZsuSC+8nJyVHnzp31wQcf6M4779Srr76q9u3bKykpSWPGjCk0/rPPPlOPHj3czsZIUmZmpo4cOeK2nPX++++rT58+8vf313PPPafx48frxx9/VIcOHbR792637eTn5yshIUHR0dF6/vnnVbt2bT300EOaNWuWevfurdatW+u5555TQECA7rzzTqWlpbnW3bVrlxYtWqS+ffvqpZde0qOPPqrvvvtOnTt31r59+y44Dz/88IPatm2rn376SY8//rhefPFFVa1aVTfddJMWLlx4wXUBXIABUOrNnDnTSDJff/21ee2110xAQIDJyckxxhhzyy23mBtuuMEYY0xMTIzp06ePa72zY87Ky8szTZo0MV27dnVrj4mJMYmJia7nkydPNlWrVjXbt293G/f4448bHx8fk56e7mrLzs42vr6+ZubMmYXqLWoxxpjjx4+b4OBgM3z4cLftHzhwwAQFBbm1JyYmGklmypQprrZjx44ZPz8/43A4zNy5c13tP//8s5FknnrqKVfbyZMnTX5+vtt+0tLSjNPpNJMmTXJrk+T2Orp162aaNm1qTp486WorKCgw7dq1M/Xq1TMALg9nPoAyZtCgQfrtt9+0ePFiHT9+XIsXLy7yIxdJ8vPzcz0+duyYMjMz1bFjR33zzTcX3Mf8+fPVsWNHXXPNNW5nLbp37678/HytXbvWNfbzzz9Xbm6uEhISCm1n+vTpWrFihdsiSStWrFBGRoYGDx7stn0fHx+1adNGX3zxRaFt/fnPf3Y9Dg4OVoMGDVS1alUNGjTI1d6gQQMFBwdr165drjan06kKFc4c6vLz83X06FH5+/urQYMGF5yHX3/9VZ9//rkGDRqk48ePu2o8evSoevXqpR07duiXX3654DwCKBoXnAJlTGhoqLp3767Zs2crJydH+fn5uvnmm4scu3jxYj3zzDPasmWLcnNzXe0Oh+OC+9ixY4e2bt2q0NDQIvsPHTrkerxkyRK1bt1a4eHhhcb94Q9/KPKC0x07dkiSunbtWuT2AwMD3Z77+voWqiUoKEg1a9Ys9FqCgoJ07Ngx1/OCggJNmzZNr7/+utLS0pSfn+/qCwkJKXL/kpSamipjjMaPH6/x48cXOebQoUOqUaPGebcBoGiED6AMGjJkiIYPH64DBw4oISFBwcHBhcb83//9n/70pz+pU6dOev311xUZGalKlSpp5syZhS7KPFdBQYF69OihcePGFdlfv3591+PPPvtMd911V7HqP3vR6vvvv6+IiIhC/edeO+Lj41Pkds7XboxxPZ4yZYrGjx+vu+++W5MnT1a1atVUoUIFjRo1qsiLZ8+tcezYserVq1eRY+rWrXve9QGcH+EDKIP69++v++67Txs2bNC8efOKHPOPf/xDvr6+Wr58udv9P2bOnHnR7depU0cnTpxQ9+7dLzju+++/V3p6uvr06VOs+uvUqSNJCgsLu+g+rtSCBQt0ww036J133nFrz8jIUPXq1c+7XlxcnCSpUqVKJV4jcLXhmg+gDPL399eMGTP09NNPF7qx11k+Pj5yOBxuHzPs3r1bixYtuuj2Bw0apJSUFC1fvrxQX0ZGhk6fPi3pzFmP8PDwYt/Lo1evXgoMDNSUKVN06tSpQv2HDx8u1vYuxMfHx+1MiHTmmpaLXa8RFhamLl266M0339T+/ftLtEbgasOZD6CMSkxMvGB/nz599NJLL6l3794aMmSIDh06pOnTp6tu3braunXrBdd99NFH9cknn6hv374aNmyYWrVqpezsbH333XdasGCBdu/ererVq2vJkiVKSEi46DUk5woMDNSMGTN0xx13qGXLlrrtttsUGhqq9PR0LVmyRO3bt9drr71WrG2eT9++fTVp0iTdddddateunb777jt9+OGHrjMbFzJ9+nR16NBBTZs21fDhwxUXF6eDBw8qJSVFe/fuveR7hQBwR/gAyqmuXbvqnXfe0dSpUzVq1CjFxsbqueee0+7duy8aPqpUqaI1a9ZoypQpmj9/vt577z0FBgaqfv36mjhxooKCgpSZmakvv/xSDz300GXVN2TIEEVFRWnq1Kl64YUXlJubqxo1aqhjx47FvobkQp544gllZ2dr9uzZmjdvnlq2bKklS5Zc0m/MNG7cWJs2bdLEiRM1a9YsHT16VGFhYWrRooUmTJjgsRqBq43DnHs+EgAuwUcffaShQ4fqyJEjCgoK8nY5AMoQrvkAcFmCg4P16quvEjwAFBtnPgAAgFWc+QAAAFYRPgAAgFWEDwAAYBXhAwAAWFXq7vNRUFCgffv2KSAgoNg3LgIAAN5hjNHx48cVFRXl+iXp8yl14WPfvn2Kjo72dhkAAOAy7NmzRzVr1rzgmFIXPgICAiSdKf7cn9UGAAClU1ZWlqKjo11/xy+k1IWPsx+1BAYGEj4AAChjLuWSCS44BQAAVhE+AACAVYQPAABgVam75gMAcHUzxuj06dPKz8/3dik4R6VKleTj43PF2yF8AABKjby8PO3fv185OTneLgVFcDgcqlmzpvz9/a9oO4QPAECpUFBQoLS0NPn4+CgqKkqVK1fmZpOliDFGhw8f1t69e1WvXr0rOgNC+AAAlAp5eXkqKChQdHS0qlSp4u1yUITQ0FDt3r1bp06duqLwwQWnAIBS5WK35ob3eOpMFP/CAADAKsIHAACwims+AACl3ssrtlvd3+ge9a3uryjDhg1TRkaGFi1a5O1SPI4zHwAAwCrCBwAAsIrwAQDAFerSpYtGjhypUaNG6ZprrlF4eLjefvttZWdn66677lJAQIDq1q2rpUuXSpLy8/N1zz33KDY2Vn5+fmrQoIGmTZt2wX0UFBQoOTnZtU7z5s21YMECV/+xY8c0dOhQhYaGys/PT/Xq1dPMmTNL9HVfLq75AFAm2P7M/3KVhmsF4B1///vfNW7cOH311VeaN2+eHnjgAS1cuFD9+/fXE088oZdffll33HGH0tPTValSJdWsWVPz589XSEiIvvzyS917772KjIzUoEGDitx+cnKyPvjgA73xxhuqV6+e1q5dq9tvv12hoaHq3Lmzxo8frx9//FFLly5V9erVlZqaqt9++83yLFwawgcAAB7QvHlz/eUvf5EkJSUlaerUqapevbqGDx8uSZowYYJmzJihrVu3qm3btpo4caJr3djYWKWkpOijjz4qMnzk5uZqypQpWrlypeLj4yVJcXFxWrdund5880117txZ6enpatGihVq3bi1Jql27dgm/4stH+AAAwAOaNWvmeuzj46OQkBA1bdrU1RYeHi5JOnTokCRp+vTpevfdd5Wenq7ffvtNeXl5uu6664rcdmpqqnJyctSjRw+39ry8PLVo0UKS9MADD2jgwIH65ptv1LNnT910001q166dJ1+ixxA+AADwgEqVKrk9dzgcbm1n7w5aUFCguXPnauzYsXrxxRcVHx+vgIAAvfDCC9q4cWOR2z5x4oQkacmSJapRo4Zbn9PplCQlJCToP//5jz777DOtWLFC3bp104gRI/TXv/7VY6/RUwgfAABYtn79erVr104PPvigq23nzp3nHd+4cWM5nU6lp6erc+fO5x0XGhqqxMREJSYmqmPHjnr00UcJHwAAQKpXr57ee+89LV++XLGxsXr//ff19ddfKzY2tsjxAQEBGjt2rEaPHq2CggJ16NBBmZmZWr9+vQIDA5WYmKgJEyaoVatWuvbaa5Wbm6vFixerUaNGll/ZpSF8AABKvfL2LaL77rtP//73v3XrrbfK4XBo8ODBevDBB11fxS3K5MmTFRoaquTkZO3atUvBwcFq2bKlnnjiCUlS5cqVlZSUpN27d8vPz08dO3bU3Llzbb2kYnEYY4y3i/i9rKwsBQUFKTMzU4GBgd4uB0ApwVdty7+TJ08qLS1NsbGx8vX19XY5KMKF/o2K8/ebm4wBAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq7i9OgCg9Psi2e7+bkgq1nBjjO677z4tWLBAx44d07///W9dd911JVPbBezevVuxsbFe2/+lInwAAHCFli1bplmzZmn16tWKi4tT9erVvV1SqUb4AADgCu3cuVORkZFq166dt0spE7jmAwCAKzBs2DCNHDlS6enpcjgcql27tgoKCpScnKzY2Fj5+fmpefPmWrBggWud1atXy+FwaPny5WrRooX8/PzUtWtXHTp0SEuXLlWjRo0UGBioIUOGKCcnx7XesmXL1KFDBwUHByskJER9+/bVzp07L1jf999/r4SEBPn7+ys8PFx33HGHjhw54upfsGCBmjZtKj8/P4WEhKh79+7Kzs72/ET9DuEDAIArMG3aNE2aNEk1a9bU/v379fXXXys5OVnvvfee3njjDf3www8aPXq0br/9dq1Zs8Zt3aefflqvvfaavvzyS+3Zs0eDBg3SK6+8otmzZ2vJkiX617/+pf/93/91jc/OztaYMWO0adMmrVq1ShUqVFD//v1VUFBQZG0ZGRnq2rWrWrRooU2bNmnZsmU6ePCgBg0aJEnav3+/Bg8erLvvvls//fSTVq9erQEDBqikf/Cej10AALgCQUFBCggIkI+PjyIiIpSbm6spU6Zo5cqVio+PlyTFxcVp3bp1evPNN9W5c2fXus8884zat28vSbrnnnuUlJSknTt3Ki4uTpJ0880364svvtBjjz0mSRo4cKDbvt99912Fhobqxx9/VJMmTQrV9tprr6lFixaaMmWK2zrR0dHavn27Tpw4odOnT2vAgAGKiYmRJDVt2tSDs1M0wgcAAB6UmpqqnJwc9ejRw609Ly9PLVq0cGtr1qyZ63F4eLiqVKniCh5n27766ivX8x07dmjChAnauHGjjhw54jrjkZ6eXmT4+Pbbb/XFF1/I39+/UN/OnTvVs2dPdevWTU2bNlWvXr3Us2dP3Xzzzbrmmmsu78VfIsIHAAAedOLECUnSkiVLVKNGDbc+p9Pp9rxSpUquxw6Hw+352bbff6TSr18/xcTE6O2331ZUVJQKCgrUpEkT5eXlnbeWfv366bnnnivUFxkZKR8fH61YsUJffvml6yOeJ598Uhs3blRsbGzxXngxED4AAPCgxo0by+l0Kj093e0jlit19OhRbdu2TW+//bY6duwoSVq3bt0F12nZsqX+8Y9/qHbt2qpYseg/+Q6HQ+3bt1f79u01YcIExcTEaOHChRozZozHaj8X4QMAAA8KCAjQ2LFjNXr0aBUUFKhDhw7KzMzU+vXrFRgYqMTExMva7jXXXKOQkBC99dZbioyMVHp6uh5//PELrjNixAi9/fbbGjx4sMaNG6dq1aopNTVVc+fO1d/+9jfXhas9e/ZUWFiYNm7cqMOHD6tRo0aXVeOlInwAAEq/Yt5x1NsmT56s0NBQJScna9euXQoODlbLli31xBNPXPY2K1SooLlz5+p//ud/1KRJEzVo0ECvvvqqunTpct51oqKitH79ej322GPq2bOncnNzFRMTo969e6tChQoKDAzU2rVr9corrygrK0sxMTF68cUXlZCQcNl1XgqHKcb3aWbMmKEZM2Zo9+7dkqRrr71WEyZMcBV58uRJPfLII5o7d65yc3PVq1cvvf766woPD7/kgrKyshQUFKTMzEwFBgYW79UAKLdeXrHd2yVcktE96nu7hDLr5MmTSktLU2xsrHx9fb1dDopwoX+j4vz9LtZ9PmrWrKmpU6dq8+bN2rRpk7p27aobb7xRP/zwgyRp9OjR+vTTTzV//nytWbNG+/bt04ABA4r50gAAQHlWrI9d+vXr5/b82Wef1YwZM7RhwwbVrFlT77zzjmbPnq2uXbtKkmbOnKlGjRppw4YNatu2reeqBgAAZdZl3+E0Pz9fc+fOVXZ2tuLj47V582adOnVK3bt3d41p2LChatWqpZSUlPNuJzc3V1lZWW4LAAAov4odPr777jv5+/vL6XTq/vvv18KFC9W4cWMdOHBAlStXVnBwsNv48PBwHThw4LzbS05OVlBQkGuJjo4u9osAAABlR7HDR4MGDbRlyxZt3LhRDzzwgBITE/Xjjz9edgFJSUnKzMx0LXv27LnsbQEAyr6S/l0RXD5P/dsU+6u2lStXVt26dSVJrVq10tdff61p06bp1ltvVV5enjIyMtzOfhw8eFARERHn3Z7T6Sx0xzcAwNXn7N09c3Jy5Ofn5+VqUJSzd1L18fG5ou1c8X0+CgoKlJubq1atWqlSpUpatWqV64dvtm3bpvT0dNcP6wAAcD4+Pj4KDg7WoUOHJElVqlSRw+HwclU4q6CgQIcPH1aVKlXOe7fUS1WstZOSkpSQkKBatWrp+PHjmj17tlavXq3ly5crKChI99xzj8aMGaNq1aopMDBQI0eOVHx8PN90AQBckrNnys8GEJQuFSpUUK1ata44FBYrfBw6dEh33nmn9u/fr6CgIDVr1kzLly93/XLfyy+/rAoVKmjgwIFuNxkDAOBSOBwORUZGKiwsTKdOnfJ2OThH5cqVVaHCZX9R1qVYdzi1gTucAigKdzgFSrcSu8MpAADAlSJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAqoreLgAAvK1t+lse21bKOx7b1HltqHXvFW9jdI/6HqgEuDyc+QAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYVdHbBQAAiqdt+ltXvpEvQq58G5fqhiR7+0KZwJkPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhVrPCRnJys66+/XgEBAQoLC9NNN92kbdu2uY3p0qWLHA6H23L//fd7tGgAAFB2FSt8rFmzRiNGjNCGDRu0YsUKnTp1Sj179lR2drbbuOHDh2v//v2u5fnnn/do0QAAoOwq1k3Gli1b5vZ81qxZCgsL0+bNm9WpUydXe5UqVRQREeGZCgEAQLlyRdd8ZGZmSpKqVavm1v7hhx+qevXqatKkiZKSkpSTk3PebeTm5iorK8ttAQAA5ddl3169oKBAo0aNUvv27dWkSRNX+5AhQxQTE6OoqCht3bpVjz32mLZt26Z//vOfRW4nOTlZEydOvNwyAABAGXPZ4WPEiBH6/vvvtW7dOrf2e++91/W4adOmioyMVLdu3bRz507VqVOn0HaSkpI0ZswY1/OsrCxFR0dfblkAAKCUu6zw8dBDD2nx4sVau3atatasecGxbdq0kSSlpqYWGT6cTqecTufllAEAAMqgYoUPY4xGjhyphQsXavXq1YqNjb3oOlu2bJEkRUZGXlaBAACgfClW+BgxYoRmz56tjz/+WAEBATpw4IAkKSgoSH5+ftq5c6dmz56tP/7xjwoJCdHWrVs1evRoderUSc2aNSuRFwAAAMqWYoWPGTNmSDpzI7HfmzlzpoYNG6bKlStr5cqVeuWVV5Sdna3o6GgNHDhQf/nLXzxWMAAAKNuK/bHLhURHR2vNmjVXVBAAACjf+G0XAABgFeEDAABYRfgAAABWXfZNxgCUfS+v2O7tEuAlKbuOWtvXhtNX9j4b3aO+hypBacGZDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGBVRW8XAOASfZHs8U22TT/q8W0CwMVw5gMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYFWxwkdycrKuv/56BQQEKCwsTDfddJO2bdvmNubkyZMaMWKEQkJC5O/vr4EDB+rgwYMeLRoAAJRdxQofa9as0YgRI7RhwwatWLFCp06dUs+ePZWdne0aM3r0aH366aeaP3++1qxZo3379mnAgAEeLxwAAJRNFYszeNmyZW7PZ82apbCwMG3evFmdOnVSZmam3nnnHc2ePVtdu3aVJM2cOVONGjXShg0b1LZtW89VDgAAyqQruuYjMzNTklStWjVJ0ubNm3Xq1Cl1797dNaZhw4aqVauWUlJSitxGbm6usrKy3BYAAFB+XXb4KCgo0KhRo9S+fXs1adJEknTgwAFVrlxZwcHBbmPDw8N14MCBIreTnJysoKAg1xIdHX25JQEAgDLgssPHiBEj9P3332vu3LlXVEBSUpIyMzNdy549e65oewAAoHQr1jUfZz300ENavHix1q5dq5o1a7raIyIilJeXp4yMDLezHwcPHlRERESR23I6nXI6nZdTBgAAKIOKdebDGKOHHnpICxcu1Oeff67Y2Fi3/latWqlSpUpatWqVq23btm1KT09XfHy8ZyoGAABlWrHOfIwYMUKzZ8/Wxx9/rICAANd1HEFBQfLz81NQUJDuuecejRkzRtWqVVNgYKBGjhyp+Ph4vukCAAAkFTN8zJgxQ5LUpUsXt/aZM2dq2LBhkqSXX35ZFSpU0MCBA5Wbm6tevXrp9ddf90ixAACg7CtW+DDGXHSMr6+vpk+frunTp192UQAAoPzit10AAIBVhA8AAGAV4QMAAFhF+AAAAFZd1k3GAACAu5dXbPd2CZdsdI/6Xt0/Zz4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVxQ4fa9euVb9+/RQVFSWHw6FFixa59Q8bNkwOh8Nt6d27t6fqBQAAZVyxw0d2draaN2+u6dOnn3dM7969tX//ftcyZ86cKyoSAACUHxWLu0JCQoISEhIuOMbpdCoiIuKyiwIAAOVXiVzzsXr1aoWFhalBgwZ64IEHdPTo0fOOzc3NVVZWltsCAADKr2Kf+biY3r17a8CAAYqNjdXOnTv1xBNPKCEhQSkpKfLx8Sk0Pjk5WRMnTvR0GQCAcuLlFdu9XQI8zOPh47bbbnM9btq0qZo1a6Y6depo9erV6tatW6HxSUlJGjNmjOt5VlaWoqOjPV0WAAAoJUr8q7ZxcXGqXr26UlNTi+x3Op0KDAx0WwAAQPlV4uFj7969Onr0qCIjI0t6VwAAoAwo9scuJ06ccDuLkZaWpi1btqhatWqqVq2aJk6cqIEDByoiIkI7d+7UuHHjVLduXfXq1cujhQMAgLKp2OFj06ZNuuGGG1zPz16vkZiYqBkzZmjr1q36+9//royMDEVFRalnz56aPHmynE6n56oGAABlVrHDR5cuXWSMOW//8uXLr6ggAABQvvHbLgAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwKqK3i4AAFC+tU1/y9slFMuGWvd6u4RyjzMfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMCqYoePtWvXql+/foqKipLD4dCiRYvc+o0xmjBhgiIjI+Xn56fu3btrx44dnqoXAACUccUOH9nZ2WrevLmmT59eZP/zzz+vV199VW+88YY2btyoqlWrqlevXjp58uQVFwsAAMq+isVdISEhQQkJCUX2GWP0yiuv6C9/+YtuvPFGSdJ7772n8PBwLVq0SLfddtuVVQsAAMo8j17zkZaWpgMHDqh79+6utqCgILVp00YpKSlFrpObm6usrCy3BQAAlF/FPvNxIQcOHJAkhYeHu7WHh4e7+s6VnJysiRMnerIMlFMvr9ju7RIu2ege9b1dAgCUWl7/tktSUpIyMzNdy549e7xdEgAAKEEeDR8RERGSpIMHD7q1Hzx40NV3LqfTqcDAQLcFAACUXx4NH7GxsYqIiNCqVatcbVlZWdq4caPi4+M9uSsAAFBGFfuajxMnTig1NdX1PC0tTVu2bFG1atVUq1YtjRo1Ss8884zq1aun2NhYjR8/XlFRUbrppps8WTcAACijih0+Nm3apBtuuMH1fMyYMZKkxMREzZo1S+PGjVN2drbuvfdeZWRkqEOHDlq2bJl8fX09VzUAACizih0+unTpImPMefsdDocmTZqkSZMmXVFhAACgfPL6t10AAMDVhfABAACsInwAAACrPHqHUwAAyrq26W95u4Ri2VDrXm+XUGyc+QAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFUeDx9PP/20HA6H29KwYUNP7wYAAJRRFUtio9dee61Wrlz5351ULJHdAACAMqhEUkHFihUVERFREpsGAABlXIlc87Fjxw5FRUUpLi5OQ4cOVXp6+nnH5ubmKisry20BAADll8fPfLRp00azZs1SgwYNtH//fk2cOFEdO3bU999/r4CAgELjk5OTNXHiRE+XcX5fJNvblyfckOTtCi5dCc9t2/SjJbp9T0p5x9sVAEDp5fEzHwkJCbrlllvUrFkz9erVS5999pkyMjL00UcfFTk+KSlJmZmZrmXPnj2eLgkAAJQiJX4laHBwsOrXr6/U1NQi+51Op5xOZ0mXAQAASokSv8/HiRMntHPnTkVGRpb0rgAAQBng8fAxduxYrVmzRrt379aXX36p/v37y8fHR4MHD/b0rgAAQBnk8Y9d9u7dq8GDB+vo0aMKDQ1Vhw4dtGHDBoWGhnp6VwAAoAzyePiYO3eupzcJAADKEX7bBQAAWEX4AAAAVhE+AACAVYQPAABgFT83C728YrtHtlOWbn8OAPAeznwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq7jDaQlJ2eWZu31uOO2Zu48CAFBacOYDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWFXR2wXgwtqmv+XtEgAA8CjOfAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq0osfEyfPl21a9eWr6+v2rRpo6+++qqkdgUAAMqQEgkf8+bN05gxY/TUU0/pm2++UfPmzdWrVy8dOnSoJHYHAADKkBIJHy+99JKGDx+uu+66S40bN9Ybb7yhKlWq6N133y2J3QEAgDLE4zcZy8vL0+bNm5WUlORqq1Chgrp3766UlJRC43Nzc5Wbm+t6npmZKUnKysrydGlnZJ8sme2eu5vfci8+CACAK3Qy+0Sx1ymJv7Fnt2mMuehYj4ePI0eOKD8/X+Hh4W7t4eHh+vnnnwuNT05O1sSJEwu1R0dHe7o0AADKodeKvcYTJVDFWcePH1dQUNAFx3j99upJSUkaM2aM63lBQYF+/fVXhYSEyOFwnHe9rKwsRUdHa8+ePQoMDLRRaqnEPJzBPPwXc3EG83AG83AG8/BfJTUXxhgdP35cUVFRFx3r8fBRvXp1+fj46ODBg27tBw8eVERERKHxTqdTTqfTrS04OPiS9xcYGHjVv5Ek5uEs5uG/mIszmIczmIczmIf/Kom5uNgZj7M8fsFp5cqV1apVK61atcrVVlBQoFWrVik+Pt7TuwMAAGVMiXzsMmbMGCUmJqp169b6wx/+oFdeeUXZ2dm66667SmJ3AACgDCmR8HHrrbfq8OHDmjBhgg4cOKDrrrtOy5YtK3QR6pVwOp166qmnCn1kc7VhHs5gHv6LuTiDeTiDeTiDefiv0jAXDnMp34kBAADwEH7bBQAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYVarDx9NPPy2Hw+G2NGzY0NXfpUuXQv3333+/FysuOb/88otuv/12hYSEyM/PT02bNtWmTZtc/cYYTZgwQZGRkfLz81P37t21Y8cOL1ZcMi42D8OGDSv0nujdu7cXKy4ZtWvXLvQ6HQ6HRowYIUk6efKkRowYoZCQEPn7+2vgwIGF7jpcHlxsHq6WY0R+fr7Gjx+v2NhY+fn5qU6dOpo8ebLbD3xdLceIS5mLq+U4cfz4cY0aNUoxMTHy8/NTu3bt9PXXX7v6vfqeMKXYU089Za699lqzf/9+13L48GFXf+fOnc3w4cPd+jMzM71Yccn49ddfTUxMjBk2bJjZuHGj2bVrl1m+fLlJTU11jZk6daoJCgoyixYtMt9++63505/+ZGJjY81vv/3mxco961LmITEx0fTu3dvtPfHrr796seqScejQIbfXuGLFCiPJfPHFF8YYY+6//34THR1tVq1aZTZt2mTatm1r2rVr592iS8DF5uFqOUY8++yzJiQkxCxevNikpaWZ+fPnG39/fzNt2jTXmKvhGGHMpc3F1XKcGDRokGncuLFZs2aN2bFjh3nqqadMYGCg2bt3rzHGu++JUh8+mjdvft7+zp07m4cffthaPd7y2GOPmQ4dOpy3v6CgwERERJgXXnjB1ZaRkWGcTqeZM2eOjRKtuNg8GHPmoHLjjTfaKagUefjhh02dOnVMQUGBycjIMJUqVTLz58939f/0009GkklJSfFilSXv9/NgzNVzjOjTp4+5++673doGDBhghg4daoy5eo4Rxlx8Loy5Oo4TOTk5xsfHxyxevNitvWXLlubJJ5/0+nuiVH/sIkk7duxQVFSU4uLiNHToUKWnp7v1f/jhh6pevbqaNGmipKQk5eTkeKnSkvPJJ5+odevWuuWWWxQWFqYWLVro7bffdvWnpaXpwIED6t69u6stKChIbdq0UUpKijdKLhEXm4ezVq9erbCwMDVo0EAPPPCAjh496oVq7cnLy9MHH3ygu+++Ww6HQ5s3b9apU6fc3g8NGzZUrVq1ytX74VznzsNZV8Mxol27dlq1apW2b98uSfr222+1bt06JSQkSLp6jhHSxefirPJ+nDh9+rTy8/Pl6+vr1u7n56d169Z5/z1R4vHmCnz22Wfmo48+Mt9++61ZtmyZiY+PN7Vq1TJZWVnGGGPefPNNs2zZMrN161bzwQcfmBo1apj+/ft7uWrPczqdxul0mqSkJPPNN9+YN9980/j6+ppZs2YZY4xZv369kWT27dvntt4tt9xiBg0a5I2SS8TF5sEYY+bMmWM+/vhjs3XrVrNw4ULTqFEjc/3115vTp097sfKSNW/ePOPj42N++eUXY4wxH374oalcuXKhcddff70ZN26c7fKsOXcejLl6jhH5+fnmscceMw6Hw1SsWNE4HA4zZcoUV//Vcoww5uJzYczVc5yIj483nTt3Nr/88os5ffq0ef/9902FChVM/fr1vf6eKNXh41zHjh0zgYGB5m9/+1uR/atWrTKS3K4BKA8qVapk4uPj3dpGjhxp2rZta4y5eg4sF5uHouzcudNIMitXrizp8rymZ8+epm/fvq7nV2v4OHceilJejxFz5swxNWvWNHPmzDFbt2417733nqlWrdpV9z8oxlx8LopSXo8TqampplOnTkaS8fHxMddff70ZOnSoadiwodffE6X+Y5ffCw4OVv369ZWamlpkf5s2bSTpvP1lVWRkpBo3buzW1qhRI9dHUBEREZJU6NsMBw8edPWVBxebh6LExcWpevXq5e49cdZ//vMfrVy5Un/+859dbREREcrLy1NGRobb2PL2fvi9ouahKOX1GPHoo4/q8ccf12233aamTZvqjjvu0OjRo5WcnCzp6jlGSBefi6KU1+NEnTp1tGbNGp04cUJ79uzRV199pVOnTikuLs7r74kyFT5OnDihnTt3KjIyssj+LVu2SNJ5+8uq9u3ba9u2bW5t27dvV0xMjCQpNjZWERERWrVqlas/KytLGzduVHx8vNVaS9LF5qEoe/fu1dGjR8vde+KsmTNnKiwsTH369HG1tWrVSpUqVXJ7P2zbtk3p6enl6v3we0XNQ1HK6zEiJydHFSq4H859fHxUUFAg6eo5RkgXn4uilPfjRNWqVRUZGaljx45p+fLluvHGG73/nijxcytX4JFHHjGrV682aWlpZv369aZ79+6mevXq5tChQyY1NdVMmjTJbNq0yaSlpZmPP/7YxMXFmU6dOnm7bI/76quvTMWKFc2zzz5rduzYYT788ENTpUoV88EHH7jGTJ061QQHB7s+x7zxxhvL3dfoLjYPx48fN2PHjjUpKSkmLS3NrFy50rRs2dLUq1fPnDx50svVe15+fr6pVauWeeyxxwr13X///aZWrVrm888/N5s2bTLx8fGFPrIqL843D1fTMSIxMdHUqFHD9fXSf/7zn6Z69epuH7NdDccIYy4+F1fTcWLZsmVm6dKlZteuXeZf//qXad68uWnTpo3Jy8szxnj3PVGqw8ett95qIiMjTeXKlU2NGjXMrbfe6vqsNj093XTq1MlUq1bNOJ1OU7duXfPoo4+Wy+/wG2PMp59+apo0aWKcTqdp2LCheeutt9z6CwoKzPjx4014eLhxOp2mW7duZtu2bV6qtuRcaB5ycnJMz549TWhoqKlUqZKJiYkxw4cPNwcOHPBixSVn+fLlRlKR/86//fabefDBB80111xjqlSpYvr372/279/vhSpL3vnm4Wo6RmRlZZmHH37Y1KpVy/j6+pq4uDjz5JNPmtzcXNeYq+UYcbG5uJqOE/PmzTNxcXGmcuXKJiIiwowYMcJkZGS4+r35nnAY87vbvgEAAJSwMnXNBwAAKPsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALDq/wHy9rJ6Q16BtgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section loads the data in by extracting it from the three data groups (AD, CN, MCI). The data is saved as '.npz' file and has been preprocessed to the desired shaped (96x96x96) and normalized to Montreal Neurological Institute (MNI) coordinate space. A Gaussian blur with $0 \\leq \\sigma \\leq 1.5$ was applied to each scan.\n",
        "\n",
        "All of this was generated with Claude Sonnet 3.7. All package imports were moved to the top to create a cohesive area for them to prevent duplicate imports."
      ],
      "metadata": {
        "id": "oumwAOxTNv16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlzheimerDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading Alzheimer's disease MRI scans from preprocessed zip files.\n",
        "    The dataset handles loading scans from multiple zip files, where each file contains\n",
        "    numpy arrays for brain scans along with metadata including patient group, ID, and age.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir=DATA_DIR, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data_dir : str\n",
        "            Directory containing the processed zip files\n",
        "        transform : callable, optional\n",
        "            Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # Find all zip files in the directory\n",
        "        zip_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
        "                    if f.endswith('.zip')]\n",
        "\n",
        "        # Load sample information from all zip files\n",
        "        for zip_path in zip_files:\n",
        "            self._load_zip_index(zip_path)\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples from {len(zip_files)} zip files\")\n",
        "\n",
        "    def _load_zip_index(self, zip_path):\n",
        "        \"\"\"\n",
        "        Load sample information from a zip file.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        zip_path : str\n",
        "            Path to the zip file\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "                for filename in zipf.namelist():\n",
        "                    if filename.endswith('.npz'):\n",
        "                        # Parse filename to extract metadata\n",
        "                        # Format: group-patient_id-image_id-age-sex.npz\n",
        "                        parts = filename.split('.')[0].split('-')\n",
        "\n",
        "                        # Handle filenames with and without age/sex information\n",
        "                        if len(parts) >= 3:\n",
        "                            group = parts[0]\n",
        "                            patient_id = parts[1]\n",
        "                            image_id = parts[2]\n",
        "\n",
        "                            # Extract age if available (default to None)\n",
        "                            age = None\n",
        "                            if len(parts) >= 4 and parts[3].replace('.', '', 1).isdigit():\n",
        "                                age = float(parts[3])\n",
        "\n",
        "                            # Extract sex if available (default to None)\n",
        "                            sex = None\n",
        "                            if len(parts) >= 5:\n",
        "                                sex = parts[4]\n",
        "\n",
        "                            # Convert group to class index\n",
        "                            label = {'CN': 0, 'MCI': 1, 'AD': 2}.get(group, -1)\n",
        "\n",
        "                            if label != -1:  # Valid group\n",
        "                                self.samples.append({\n",
        "                                    'zip_path': zip_path,\n",
        "                                    'filename': filename,\n",
        "                                    'group': group,\n",
        "                                    'label': label,\n",
        "                                    'patient_id': patient_id,\n",
        "                                    'image_id': image_id,\n",
        "                                    'age': age,\n",
        "                                    'sex': sex\n",
        "                                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading zip file {zip_path}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a sample from the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        idx : int\n",
        "            Index of the sample\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dictionary containing the sample data and metadata\n",
        "        \"\"\"\n",
        "        sample_info = self.samples[idx]\n",
        "\n",
        "        # Load the actual data from the zip file\n",
        "        with zipfile.ZipFile(sample_info['zip_path'], 'r') as zipf:\n",
        "            with zipf.open(sample_info['filename']) as f:\n",
        "                buffer = io.BytesIO(f.read())\n",
        "                data_npz = np.load(buffer)\n",
        "                scan_data = data_npz['data']\n",
        "\n",
        "        # Convert to tensor\n",
        "        scan_tensor = torch.from_numpy(scan_data).float()\n",
        "\n",
        "        # Add channel dimension if missing\n",
        "        if scan_tensor.dim() == 3:\n",
        "            scan_tensor = scan_tensor.unsqueeze(0)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform:\n",
        "            scan_tensor = self.transform(scan_tensor)\n",
        "\n",
        "        # Create result dictionary\n",
        "        result = {\n",
        "            'scan': scan_tensor,\n",
        "            'label': sample_info['label'],\n",
        "            'group': sample_info['group'],\n",
        "            'patient_id': sample_info['patient_id'],\n",
        "            'image_id': sample_info['image_id']\n",
        "        }\n",
        "\n",
        "        # Add age if available\n",
        "        if sample_info['age'] is not None:\n",
        "            result['age'] = torch.tensor(sample_info['age']).float()\n",
        "\n",
        "        # Add sex if available\n",
        "        if sample_info['sex'] is not None:\n",
        "            # Convert sex to numeric: 0 for male (M), 1 for female (F)\n",
        "            sex_value = 1 if sample_info['sex'].upper() == 'F' else 0\n",
        "            result['sex'] = torch.tensor(sex_value).long()\n",
        "\n",
        "        return result\n",
        "\n",
        "def create_data_loaders(data_dir=DATA_DIR, batch_size=4, num_workers=4,\n",
        "                       train_val_test_split=(0.7, 0.15, 0.15), transform=None,\n",
        "                       shuffle=True, random_seed=42):\n",
        "    \"\"\"\n",
        "    Create data loaders for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_dir : str\n",
        "        Directory containing the processed zip files\n",
        "    batch_size : int\n",
        "        Batch size for the data loaders\n",
        "    num_workers : int\n",
        "        Number of workers for the data loaders\n",
        "    train_val_test_split : tuple\n",
        "        Proportions of training, validation, and test sets\n",
        "    transform : callable, optional\n",
        "        Optional transform to be applied on the samples\n",
        "    shuffle : bool\n",
        "        Whether to shuffle the datasets\n",
        "    random_seed : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (train_loader, val_loader, test_loader)\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Load the full dataset\n",
        "    full_dataset = AlzheimerDataset(data_dir=data_dir, transform=transform)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(dataset_size * train_val_test_split[0])\n",
        "    val_size = int(dataset_size * train_val_test_split[1])\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    # Split the dataset\n",
        "    indices = list(range(dataset_size))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    # Create samplers\n",
        "    train_sampler = data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = data.SubsetRandomSampler(val_indices)\n",
        "    test_sampler = data.SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = data.DataLoader(\n",
        "        full_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    val_loader = data.DataLoader(\n",
        "        full_dataset, batch_size=batch_size, sampler=val_sampler,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    test_loader = data.DataLoader(\n",
        "        full_dataset, batch_size=batch_size, sampler=test_sampler,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    print(f\"Created data loaders with {train_size} training, {val_size} validation, \"\n",
        "          f\"and {test_size} test samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "WNr6NRKeQfiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next sections will load the PyTorch packages necessary for the CNN model and build the classes needed for the age encoding and model itself.\n",
        "\n",
        "All of this was generated with Claude Sonnet 3.7. All package imports were moved to the top to create a cohesive area for them to prevent duplicate imports."
      ],
      "metadata": {
        "id": "pjPa9DLKLMHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgeEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Age encoding class inspired by positional encoding in the transformer model.\n",
        "    Encodes age values into vectors that can be integrated with CNN features.\n",
        "\n",
        "    The encoding is defined as:\n",
        "    AE(age, 2i) = sin(age/10000^(2i/d_model))\n",
        "    AE(age, 2i+1) = cos(age/10000^(2i/d_model))\n",
        "\n",
        "    Where:\n",
        "    - age is the patient's age value (rounded to 0.5 decimal places)\n",
        "    - i is the dimension index\n",
        "    - d_model is the size of the encoding\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=512, age_min=0, age_max=120, age_step=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the age encoding module.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        d_model : int\n",
        "            Size of the age encoding vector\n",
        "        age_min : float\n",
        "            Minimum possible age value\n",
        "        age_max : float\n",
        "            Maximum possible age value\n",
        "        age_step : float\n",
        "            Step size for discretizing age values\n",
        "        \"\"\"\n",
        "        super(AgeEncoding, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.age_min = age_min\n",
        "        self.age_max = age_max\n",
        "        self.age_step = age_step\n",
        "\n",
        "        # Create encoder layers to transform age encoding to match visual representation\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(d_model, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, 1024)\n",
        "        )\n",
        "\n",
        "        # Pre-compute all possible age encodings\n",
        "        self.num_ages = int((age_max - age_min) / age_step) + 1\n",
        "        self.register_buffer('age_encodings', self._create_age_encodings())\n",
        "\n",
        "    def _create_age_encodings(self):\n",
        "        \"\"\"\n",
        "        Pre-compute all possible age encodings.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            Tensor of shape (num_ages, d_model) containing all possible age encodings\n",
        "        \"\"\"\n",
        "        # Create all possible age values\n",
        "        ages = torch.arange(self.age_min, self.age_max + self.age_step, self.age_step)\n",
        "\n",
        "        # Create position encodings for all ages\n",
        "        position_encodings = torch.zeros(len(ages), self.d_model)\n",
        "\n",
        "        # Apply sinusoidal encoding\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model))\n",
        "\n",
        "        for idx, age in enumerate(ages):\n",
        "            position_encodings[idx, 0::2] = torch.sin(age * div_term)\n",
        "            position_encodings[idx, 1::2] = torch.cos(age * div_term)\n",
        "\n",
        "        return position_encodings\n",
        "\n",
        "    def forward(self, age):\n",
        "        \"\"\"\n",
        "        Encode age values into vectors.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        age : torch.Tensor\n",
        "            Tensor of age values of shape (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            Encoded age vectors of shape (batch_size, 1024)\n",
        "        \"\"\"\n",
        "        # Discretize age values to the nearest step\n",
        "        age_indices = ((age - self.age_min) / self.age_step).round().long()\n",
        "\n",
        "        # Clamp indices to valid range\n",
        "        age_indices = torch.clamp(age_indices, 0, self.num_ages - 1)\n",
        "\n",
        "        # Get corresponding encodings\n",
        "        encodings = self.age_encodings[age_indices]\n",
        "\n",
        "        # Transform the encodings to match visual representation\n",
        "        return self.encoder(encodings)\n",
        "\n",
        "class AlzheimerCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Convolutional Neural Network for Alzheimer's disease detection.\n",
        "    Distinguishes between Cognitively Normal (CN), Mild Cognitive Impairment (MCI),\n",
        "    and Alzheimer's Disease (AD) using structural brain MRI scans.\n",
        "\n",
        "    Key features of this architecture:\n",
        "    - Instance normalization instead of batch normalization\n",
        "    - Small-sized kernels in first layer to avoid early spatial downsampling\n",
        "    - Wide architecture with large numbers of filters\n",
        "    - Age information integration\n",
        "    \"\"\"\n",
        "    def __init__(self, widening_factor=8, use_age=True):\n",
        "        \"\"\"\n",
        "        Initialize the CNN model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        widening_factor : int\n",
        "            Factor to increase the number of filters in each layer\n",
        "        use_age : bool\n",
        "            Whether to incorporate age information\n",
        "        \"\"\"\n",
        "        super(AlzheimerCNN, self).__init__()\n",
        "\n",
        "        self.widening_factor = widening_factor\n",
        "        self.use_age = use_age\n",
        "\n",
        "        # Block 1: Small kernel size (1x1x1) to prevent early spatial downsampling\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv3d(1, 4 * widening_factor, kernel_size=1, stride=1, padding=0, dilation=1),\n",
        "            nn.InstanceNorm3d(4 * widening_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv3d(4 * widening_factor, 32 * widening_factor, kernel_size=3, stride=1, padding=0, dilation=2),\n",
        "            nn.InstanceNorm3d(32 * widening_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv3d(32 * widening_factor, 64 * widening_factor, kernel_size=5, stride=1, padding=2, dilation=2),\n",
        "            nn.InstanceNorm3d(64 * widening_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        # Block 4\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv3d(64 * widening_factor, 64 * widening_factor, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.InstanceNorm3d(64 * widening_factor),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=5, stride=2)\n",
        "        )\n",
        "\n",
        "        # Age encoding if enabled\n",
        "        if use_age:\n",
        "            self.age_encoder = AgeEncoding(d_model=512)\n",
        "\n",
        "        # Calculate the feature size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 96, 96, 96)\n",
        "        self.feature_size = self._calculate_output_shape(dummy_input)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.feature_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 3)  # 3 output classes: CN, MCI, AD\n",
        "\n",
        "        print(f\"Feature size after convolutions: {self.feature_size}\")\n",
        "\n",
        "    def _calculate_output_shape(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the output shape of the convolutional layers.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : torch.Tensor\n",
        "            Dummy input of shape (batch_size, channels, height, width, depth)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int\n",
        "            Total number of features after flattening\n",
        "        \"\"\"\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        return x.numel()\n",
        "\n",
        "    def forward(self, x, age=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : torch.Tensor\n",
        "            Input 3D brain scan tensor of shape (batch_size, 1, 96, 96, 96)\n",
        "        age : torch.Tensor or None\n",
        "            Age values tensor of shape (batch_size,), required if use_age=True\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            Output logits of shape (batch_size, 3)\n",
        "        \"\"\"\n",
        "        # Ensure input has channel dimension\n",
        "        if x.dim() == 4:\n",
        "            x = x.unsqueeze(1)  # Add channel dimension if not present\n",
        "\n",
        "        # Forward through convolutional blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "\n",
        "        # Flatten features\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # FC1\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        # Add age information if enabled\n",
        "        if self.use_age and age is not None:\n",
        "            age_encoding = self.age_encoder(age)\n",
        "            x = x + age_encoding\n",
        "\n",
        "        # FC2 (Output layer)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UMB3Jmc-Kh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section creates the ML pipeline.\n",
        "\n",
        "All of this was generated with Claude Sonnet 3.7. All package imports were moved to the top to create a cohesive area for them to prevent duplicate imports."
      ],
      "metadata": {
        "id": "k9dLpzn8Vru1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlzheimerPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline for training, testing, and visualizing the Alzheimer's CNN model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_loader=None, val_loader=None, test_loader=None,\n",
        "                 device=None, output_dir=\"results\"):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : AlzheimerCNN\n",
        "            The model to test and visualize\n",
        "        train_loader : DataLoader\n",
        "            DataLoader for training data\n",
        "        val_loader : DataLoader\n",
        "            DataLoader for validation data\n",
        "        test_loader : DataLoader\n",
        "            DataLoader for test data\n",
        "        device : torch.device\n",
        "            Device to run the model on (CPU or GPU)\n",
        "        output_dir : str\n",
        "            Directory to save results\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        # Set device\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Move model to device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Create output directory\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Class names for plots\n",
        "        self.class_names = [\"CN\", \"MCI\", \"AD\"]\n",
        "\n",
        "        # Tensorboard writer\n",
        "        self.writer = SummaryWriter(os.path.join(output_dir, \"tensorboard\"))\n",
        "\n",
        "        # Initialize metrics\n",
        "        self.metrics = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_acc': [],\n",
        "            'test_acc': None,\n",
        "            'confusion_matrix': None,\n",
        "            'classification_report': None,\n",
        "            'roc_auc': None\n",
        "        }\n",
        "\n",
        "    def train(self, num_epochs=50, lr=0.001, weight_decay=1e-4, patience=10):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_epochs : int\n",
        "            Number of epochs to train\n",
        "        lr : float\n",
        "            Learning rate\n",
        "        weight_decay : float\n",
        "            L2 regularization strength\n",
        "        patience : int\n",
        "            Patience for early stopping\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Training metrics\n",
        "        \"\"\"\n",
        "        if self.train_loader is None or self.val_loader is None:\n",
        "            raise ValueError(\"Train and validation loaders must be provided for training\")\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=patience//2\n",
        "\t\t)\n",
        "\n",
        "        # Early stopping variables\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            train_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "            for batch in train_bar:\n",
        "                # Get the data\n",
        "                scans = batch['scan'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                # Get age if available\n",
        "                age = batch.get('age')\n",
        "                if age is not None:\n",
        "                    age = age.to(self.device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(scans, age)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track metrics\n",
        "                train_loss += loss.item() * scans.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Update progress bar\n",
        "                train_bar.set_postfix(\n",
        "                    loss=f\"{loss.item():.4f}\",\n",
        "                    acc=f\"{100. * train_correct / train_total:.2f}%\"\n",
        "                )\n",
        "\n",
        "            train_loss = train_loss / train_total\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "\n",
        "            # Log training metrics\n",
        "            self.metrics['train_loss'].append(train_loss)\n",
        "            self.metrics['train_acc'].append(train_acc)\n",
        "            self.writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            self.writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_bar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "                for batch in val_bar:\n",
        "                    # Get the data\n",
        "                    scans = batch['scan'].to(self.device)\n",
        "                    labels = batch['label'].to(self.device)\n",
        "\n",
        "                    # Get age if available\n",
        "                    age = batch.get('age')\n",
        "                    if age is not None:\n",
        "                        age = age.to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(scans, age)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Track metrics\n",
        "                    val_loss += loss.item() * scans.size(0)\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    # Update progress bar\n",
        "                    val_bar.set_postfix(\n",
        "                        loss=f\"{loss.item():.4f}\",\n",
        "                        acc=f\"{100. * val_correct / val_total:.2f}%\"\n",
        "                    )\n",
        "\n",
        "            val_loss = val_loss / val_total\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "\n",
        "            # Log validation metrics\n",
        "            self.metrics['val_loss'].append(val_loss)\n",
        "            self.metrics['val_acc'].append(val_acc)\n",
        "            self.writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "            self.writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = self.model.state_dict().copy()\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save the best model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': best_model_state,\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'val_loss': best_val_loss,\n",
        "                    'val_acc': val_acc\n",
        "                }, os.path.join(self.output_dir, \"best_model.pth\"))\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Load the best model\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "            print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Plot training curves\n",
        "        self.plot_training_curves()\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"\n",
        "        Test the model on the test set.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Test metrics\n",
        "        \"\"\"\n",
        "        if self.test_loader is None:\n",
        "            raise ValueError(\"Test loader must be provided for testing\")\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Initialize metrics\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        # Initialize arrays for confusion matrix and ROC curves\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        y_score = []\n",
        "\n",
        "        # Test the model\n",
        "        with torch.no_grad():\n",
        "            test_bar = tqdm(self.test_loader, desc=\"Testing\")\n",
        "            for batch in test_bar:\n",
        "                # Get the data\n",
        "                scans = batch['scan'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                # Get age if available\n",
        "                age = batch.get('age')\n",
        "                if age is not None:\n",
        "                    age = age.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(scans, age)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Track metrics\n",
        "                test_loss += loss.item() * scans.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Store predictions for confusion matrix and ROC curves\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "                y_score.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                test_bar.set_postfix(\n",
        "                    loss=f\"{loss.item():.4f}\",\n",
        "                    acc=f\"{100. * test_correct / test_total:.2f}%\"\n",
        "                )\n",
        "\n",
        "        # Calculate test metrics\n",
        "        test_loss = test_loss / test_total\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "        y_score = np.array(y_score)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        self.metrics['confusion_matrix'] = cm\n",
        "\n",
        "        # Compute ROC AUC\n",
        "        roc_auc = {}\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            # One-vs-rest ROC curve\n",
        "            fpr, tpr, _ = roc_curve((y_true == i).astype(int), y_score[:, i])\n",
        "            roc_auc[class_name] = auc(fpr, tpr)\n",
        "\n",
        "        self.metrics['roc_auc'] = roc_auc\n",
        "\n",
        "        # Compute classification report\n",
        "        report = classification_report(\n",
        "            y_true, y_pred,\n",
        "            target_names=self.class_names,\n",
        "            output_dict=True\n",
        "        )\n",
        "        self.metrics['classification_report'] = report\n",
        "\n",
        "        # Log test metrics\n",
        "        self.metrics['test_loss'] = test_loss\n",
        "        self.metrics['test_acc'] = test_acc\n",
        "\n",
        "        # Print test summary\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=self.class_names))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        self.plot_confusion_matrix(cm)\n",
        "\n",
        "        # Plot ROC curves\n",
        "        self.plot_roc_curves(y_true, y_score)\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"\n",
        "        Plot training and validation loss and accuracy curves.\n",
        "        \"\"\"\n",
        "        # Create figure\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Get epochs\n",
        "        epochs = range(1, len(self.metrics['train_loss']) + 1)\n",
        "\n",
        "        # Plot loss\n",
        "        ax1.plot(epochs, self.metrics['train_loss'], 'b-', label='Train')\n",
        "        ax1.plot(epochs, self.metrics['val_loss'], 'r-', label='Validation')\n",
        "        ax1.set_title('Loss Curves')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax2.plot(epochs, self.metrics['train_acc'], 'b-', label='Train')\n",
        "        ax2.plot(epochs, self.metrics['val_acc'], 'r-', label='Validation')\n",
        "        ax2.set_title('Accuracy Curves')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy (%)')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.output_dir, \"training_curves.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_confusion_matrix(self, cm):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cm : numpy.ndarray\n",
        "            Confusion matrix\n",
        "        \"\"\"\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(8, 6))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        sns.heatmap(\n",
        "            cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\n",
        "\n",
        "        # Set labels\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "\n",
        "        # Save figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"confusion_matrix.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_roc_curves(self, y_true, y_score):\n",
        "        \"\"\"\n",
        "        Plot ROC curves.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : numpy.ndarray\n",
        "            True labels\n",
        "        y_score : numpy.ndarray\n",
        "            Predicted scores\n",
        "        \"\"\"\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(8, 6))\n",
        "\n",
        "        # Plot ROC curves for each class\n",
        "        colors = ['blue', 'red', 'green']\n",
        "        for i, (class_name, color) in enumerate(zip(self.class_names, colors)):\n",
        "            # One-vs-rest ROC curve\n",
        "            fpr, tpr, _ = roc_curve((y_true == i).astype(int), y_score[:, i])\n",
        "            auc_score = self.metrics['roc_auc'][class_name]\n",
        "\n",
        "            plt.plot(\n",
        "                fpr, tpr, color=color, lw=2,\n",
        "                label=f'{class_name} (AUC = {auc_score:.3f})'\n",
        "            )\n",
        "\n",
        "        # Plot random guessing line\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "        # Set labels\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Set limits\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "\n",
        "        # Save figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"roc_curves.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def visualize_saliency_maps(self, num_samples=5):\n",
        "        \"\"\"\n",
        "        Visualize saliency maps for sample inputs.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_samples : int\n",
        "            Number of samples to visualize\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Saliency maps information\n",
        "        \"\"\"\n",
        "        if self.test_loader is None:\n",
        "            raise ValueError(\"Test loader must be provided for visualization\")\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Create directory for saliency maps\n",
        "        saliency_dir = os.path.join(self.output_dir, \"saliency_maps\")\n",
        "        os.makedirs(saliency_dir, exist_ok=True)\n",
        "\n",
        "        # Get samples from each class\n",
        "        class_samples = {class_idx: [] for class_idx in range(len(self.class_names))}\n",
        "\n",
        "        # Collect samples\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_loader:\n",
        "                # Get the data\n",
        "                scans = batch['scan']\n",
        "                labels = batch['label'].numpy()\n",
        "\n",
        "                # Store samples for each class\n",
        "                for i, label in enumerate(labels):\n",
        "                    if len(class_samples[label]) < num_samples:\n",
        "                        class_samples[label].append({\n",
        "                            'scan': scans[i].clone(),\n",
        "                            'label': label,\n",
        "                            'patient_id': batch['patient_id'][i],\n",
        "                            'age': batch.get('age')[i].item() if batch.get('age') is not None else None\n",
        "                        })\n",
        "\n",
        "                # Check if we have enough samples\n",
        "                if all(len(samples) >= num_samples for samples in class_samples.values()):\n",
        "                    break\n",
        "\n",
        "        # Generate saliency maps for each class\n",
        "        all_saliency_maps = {}\n",
        "\n",
        "        for class_idx, samples in class_samples.items():\n",
        "            class_name = self.class_names[class_idx]\n",
        "            class_maps = []\n",
        "\n",
        "            for i, sample in enumerate(samples):\n",
        "                # Prepare input\n",
        "                scan = sample['scan'].unsqueeze(0).to(self.device).requires_grad_(True)\n",
        "\n",
        "                # Prepare age if available\n",
        "                if sample['age'] is not None:\n",
        "                    age = torch.tensor([sample['age']]).float().to(self.device)\n",
        "                else:\n",
        "                    age = None\n",
        "\n",
        "                # Forward pass\n",
        "                self.model.zero_grad()\n",
        "                output = self.model(scan, age)\n",
        "\n",
        "                # Get score for true class\n",
        "                score = output[0][class_idx]\n",
        "\n",
        "                # Backward pass to compute gradients w.r.t. input\n",
        "                score.backward()\n",
        "\n",
        "                # Get gradients\n",
        "                gradients = scan.grad.abs().detach().cpu().numpy()[0, 0]  # Remove batch and channel dims\n",
        "\n",
        "                # Normalize gradients for visualization\n",
        "                gradients = (gradients - gradients.min()) / (gradients.max() - gradients.min() + 1e-8)\n",
        "\n",
        "                # Get original scan\n",
        "                original_scan = scan.detach().cpu().numpy()[0, 0]  # Remove batch and channel dims\n",
        "\n",
        "                # Store maps\n",
        "                class_maps.append({\n",
        "                    'patient_id': sample['patient_id'],\n",
        "                    'original_scan': original_scan,\n",
        "                    'saliency_map': gradients,\n",
        "                    'age': sample['age']\n",
        "                })\n",
        "\n",
        "                # Visualize middle slices of the brain\n",
        "                fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "                # Get middle slices\n",
        "                x_mid = original_scan.shape[0] // 2\n",
        "                y_mid = original_scan.shape[1] // 2\n",
        "                z_mid = original_scan.shape[2] // 2\n",
        "\n",
        "                # Plot original scan\n",
        "                axes[0, 0].imshow(original_scan[x_mid, :, :], cmap='gray')\n",
        "                axes[0, 0].set_title('Original (Sagittal)')\n",
        "                axes[0, 0].axis('off')\n",
        "\n",
        "                axes[0, 1].imshow(original_scan[:, y_mid, :], cmap='gray')\n",
        "                axes[0, 1].set_title('Original (Coronal)')\n",
        "                axes[0, 1].axis('off')\n",
        "\n",
        "                axes[0, 2].imshow(original_scan[:, :, z_mid], cmap='gray')\n",
        "                axes[0, 2].set_title('Original (Axial)')\n",
        "                axes[0, 2].axis('off')\n",
        "\n",
        "                # Plot saliency maps\n",
        "                axes[1, 0].imshow(gradients[x_mid, :, :], cmap='hot')\n",
        "                axes[1, 0].set_title('Saliency (Sagittal)')\n",
        "                axes[1, 0].axis('off')\n",
        "\n",
        "                axes[1, 1].imshow(gradients[:, y_mid, :], cmap='hot')\n",
        "                axes[1, 1].set_title('Saliency (Coronal)')\n",
        "                axes[1, 1].axis('off')\n",
        "\n",
        "                axes[1, 2].imshow(gradients[:, :, z_mid], cmap='hot')\n",
        "                axes[1, 2].set_title('Saliency (Axial)')\n",
        "                axes[1, 2].axis('off')\n",
        "\n",
        "                # Set title\n",
        "                age_str = f\", Age: {sample['age']}\" if sample['age'] is not None else \"\"\n",
        "                plt.suptitle(f\"Patient: {sample['patient_id']}, Class: {class_name}{age_str}\")\n",
        "\n",
        "                # Save figure\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(saliency_dir, f\"{class_name}_{sample['patient_id']}.png\"), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "            # Store maps for this class\n",
        "            all_saliency_maps[class_name] = class_maps\n",
        "\n",
        "        # Create aggregated saliency maps for each class\n",
        "        for class_idx, class_name in enumerate(self.class_names):\n",
        "            # Get maps for this class\n",
        "            maps = [sample['saliency_map'] for sample in all_saliency_maps[class_name]]\n",
        "\n",
        "            # Average maps\n",
        "            avg_map = np.mean(maps, axis=0)\n",
        "\n",
        "            # Normalize\n",
        "            avg_map = (avg_map - avg_map.min()) / (avg_map.max() - avg_map.min() + 1e-8)\n",
        "\n",
        "            # Visualize middle slices\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Get middle slices\n",
        "            x_mid = avg_map.shape[0] // 2\n",
        "            y_mid = avg_map.shape[1] // 2\n",
        "            z_mid = avg_map.shape[2] // 2\n",
        "\n",
        "            # Plot average saliency maps\n",
        "            axes[0].imshow(avg_map[x_mid, :, :], cmap='hot')\n",
        "            axes[0].set_title('Sagittal')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            axes[1].imshow(avg_map[:, y_mid, :], cmap='hot')\n",
        "            axes[1].set_title('Coronal')\n",
        "            axes[1].axis('off')\n",
        "\n",
        "            axes[2].imshow(avg_map[:, :, z_mid], cmap='hot')\n",
        "            axes[2].set_title('Axial')\n",
        "            axes[2].axis('off')\n",
        "\n",
        "            # Set title\n",
        "            plt.suptitle(f\"Average Saliency Map for {class_name}\")\n",
        "\n",
        "            # Save figure\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(saliency_dir, f\"avg_{class_name}.png\"), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "        return all_saliency_maps\n",
        "\n",
        "    def export_metrics(self):\n",
        "        \"\"\"\n",
        "        Export metrics to CSV files.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Paths to exported metrics\n",
        "        \"\"\"\n",
        "        # Create metrics directory\n",
        "        metrics_dir = os.path.join(self.output_dir, \"metrics\")\n",
        "        os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "        # Export training metrics\n",
        "        if self.metrics['train_loss']:\n",
        "            train_df = pd.DataFrame({\n",
        "                'epoch': range(1, len(self.metrics['train_loss']) + 1),\n",
        "                'train_loss': self.metrics['train_loss'],\n",
        "                'val_loss': self.metrics['val_loss'],\n",
        "                'train_acc': self.metrics['train_acc'],\n",
        "                'val_acc': self.metrics['val_acc']\n",
        "            })\n",
        "            train_path = os.path.join(metrics_dir, \"training_metrics.csv\")\n",
        "            train_df.to_csv(train_path, index=False)\n",
        "        else:\n",
        "            train_path = None\n",
        "\n",
        "        # Export test metrics\n",
        "        if self.metrics['test_acc'] is not None:\n",
        "            # Classification report\n",
        "            if self.metrics['classification_report']:\n",
        "                report_df = pd.DataFrame(self.metrics['classification_report']).transpose()\n",
        "                report_path = os.path.join(metrics_dir, \"classification_report.csv\")\n",
        "                report_df.to_csv(report_path)\n",
        "            else:\n",
        "                report_path = None\n",
        "\n",
        "            # Confusion matrix\n",
        "            if self.metrics['confusion_matrix'] is not None:\n",
        "                cm_df = pd.DataFrame(\n",
        "                    self.metrics['confusion_matrix'],\n",
        "                    index=self.class_names,\n",
        "                    columns=self.class_names\n",
        "                )\n",
        "                cm_path = os.path.join(metrics_dir, \"confusion_matrix.csv\")\n",
        "                cm_df.to_csv(cm_path)\n",
        "            else:\n",
        "                cm_path = None\n",
        "\n",
        "            # ROC AUC\n",
        "            if self.metrics['roc_auc']:\n",
        "                roc_df = pd.DataFrame({\n",
        "                    'class': list(self.metrics['roc_auc'].keys()),\n",
        "                    'auc': list(self.metrics['roc_auc'].values())\n",
        "                })\n",
        "                roc_path = os.path.join(metrics_dir, \"roc_auc.csv\")\n",
        "                roc_df.to_csv(roc_path, index=False)\n",
        "            else:\n",
        "                roc_path = None\n",
        "\n",
        "            # Test summary\n",
        "            test_summary = pd.DataFrame({\n",
        "                'metric': ['test_loss', 'test_acc'],\n",
        "                'value': [self.metrics['test_loss'], self.metrics['test_acc']]\n",
        "            })\n",
        "            test_path = os.path.join(metrics_dir, \"test_summary.csv\")\n",
        "            test_summary.to_csv(test_path, index=False)\n",
        "        else:\n",
        "            report_path = None\n",
        "            cm_path = None\n",
        "            roc_path = None\n",
        "            test_path = None\n",
        "\n",
        "        return {\n",
        "            'training_metrics': train_path,\n",
        "            'test_summary': test_path,\n",
        "            'classification_report': report_path,\n",
        "            'confusion_matrix': cm_path,\n",
        "            'roc_auc': roc_path\n",
        "        }\n",
        "\n",
        "    def predict(self, scan, age=None):\n",
        "        \"\"\"\n",
        "        Make a prediction for a single scan.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        scan : torch.Tensor or numpy.ndarray\n",
        "            Input scan of shape (1, 96, 96, 96) or (96, 96, 96)\n",
        "        age : float or None\n",
        "            Age of the patient\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Prediction results\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Convert scan to tensor if needed\n",
        "        if isinstance(scan, np.ndarray):\n",
        "            scan = torch.from_numpy(scan).float()\n",
        "\n",
        "        # Add batch and channel dimensions if needed\n",
        "        if scan.dim() == 3:\n",
        "            scan = scan.unsqueeze(0).unsqueeze(0)\n",
        "        elif scan.dim() == 4:\n",
        "            scan = scan.unsqueeze(0)\n",
        "\n",
        "        # Move to device\n",
        "        scan = scan.to(self.device)\n",
        "\n",
        "        # Prepare age if available\n",
        "        if age is not None:\n",
        "            age = torch.tensor([age]).float().to(self.device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(scan, age)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            predicted_class = outputs.argmax(1).item()\n",
        "\n",
        "        # Get prediction details\n",
        "        prediction = {\n",
        "            'class_index': predicted_class,\n",
        "            'class_name': self.class_names[predicted_class],\n",
        "            'probabilities': {\n",
        "                class_name: prob.item()\n",
        "                for class_name, prob in zip(self.class_names, probabilities[0])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def visualize_prediction(self, scan, age=None, save_path=None):\n",
        "        \"\"\"\n",
        "        Visualize a prediction for a single scan.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        scan : torch.Tensor or numpy.ndarray\n",
        "            Input scan of shape (1, 96, 96, 96) or (96, 96, 96)\n",
        "        age : float or None\n",
        "            Age of the patient\n",
        "        save_path : str or None\n",
        "            Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Prediction results\n",
        "        \"\"\"\n",
        "        # Convert scan to numpy if needed\n",
        "        if isinstance(scan, torch.Tensor):\n",
        "            scan_np = scan.detach().cpu().numpy()\n",
        "            if scan_np.ndim == 4:\n",
        "                scan_np = scan_np[0]  # Remove channel dimension\n",
        "        else:\n",
        "            scan_np = scan\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = self.predict(scan, age)\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # Get middle slices\n",
        "        x_mid = scan_np.shape[0] // 2\n",
        "        y_mid = scan_np.shape[1] // 2\n",
        "        z_mid = scan_np.shape[2] // 2\n",
        "\n",
        "        # Plot middle slices\n",
        "        axes[0].imshow(scan_np[x_mid, :, :], cmap='gray')\n",
        "        axes[0].set_title('Sagittal')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(scan_np[:, y_mid, :], cmap='gray')\n",
        "        axes[1].set_title('Coronal')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        axes[2].imshow(scan_np[:, :, z_mid], cmap='gray')\n",
        "        axes[2].set_title('Axial')\n",
        "        axes[2].axis('off')\n",
        "\n",
        "        # Set title\n",
        "        pred_class = prediction['class_name']\n",
        "        probabilities = prediction['probabilities']\n",
        "        prob_str = ', '.join([f\"{k}: {v:.2f}\" for k, v in probabilities.items()])\n",
        "        age_str = f\", Age: {age}\" if age is not None else \"\"\n",
        "        plt.suptitle(f\"Prediction: {pred_class}{age_str}\\nProbabilities: {prob_str}\")\n",
        "\n",
        "        # Save figure\n",
        "        if save_path:\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path, dpi=300)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def generate_gradcam(self, scan, target_class=None, age=None, save_path=None):\n",
        "        \"\"\"\n",
        "        Generate Grad-CAM visualization for a scan.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        scan : torch.Tensor or numpy.ndarray\n",
        "            Input scan\n",
        "        target_class : int or None\n",
        "            Target class index, if None uses the predicted class\n",
        "        age : float or None\n",
        "            Age of the patient\n",
        "        save_path : str or None\n",
        "            Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Grad-CAM results\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Convert scan to tensor if needed\n",
        "        if isinstance(scan, np.ndarray):\n",
        "            scan = torch.from_numpy(scan).float()\n",
        "\n",
        "        # Add batch and channel dimensions if needed\n",
        "        if scan.dim() == 3:\n",
        "            scan = scan.unsqueeze(0).unsqueeze(0)\n",
        "        elif scan.dim() == 4:\n",
        "            scan = scan.unsqueeze(0)\n",
        "\n",
        "        # Clone the scan and make it require gradients\n",
        "        scan = scan.clone().to(self.device).requires_grad_(True)\n",
        "\n",
        "        # Prepare age if available\n",
        "        if age is not None:\n",
        "            age = torch.tensor([age]).float().to(self.device)\n",
        "\n",
        "        # Get activation maps\n",
        "        activation_maps = []\n",
        "        gradients = []\n",
        "\n",
        "        # Forward hooks\n",
        "        def save_activation(module, input, output):\n",
        "            activation_maps.append(output.detach().cpu())\n",
        "\n",
        "        # Backward hooks\n",
        "        def save_gradient(module, grad_input, grad_output):\n",
        "            gradients.append(grad_output[0].detach().cpu())\n",
        "\n",
        "        # Register hooks\n",
        "        handles = []\n",
        "        target_layer = self.model.block4[-3]  # Conv3D in the last block\n",
        "        handles.append(target_layer.register_forward_hook(save_activation))\n",
        "        handles.append(target_layer.register_full_backward_hook(save_gradient))\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(scan, age)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get predicted class if target_class is not specified\n",
        "        if target_class is None:\n",
        "            target_class = outputs.argmax(1).item()\n",
        "\n",
        "        # Get score for target class\n",
        "        score = outputs[0][target_class]\n",
        "\n",
        "        # Backward pass\n",
        "        self.model.zero_grad()\n",
        "        score.backward()\n",
        "\n",
        "        # Remove hooks\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "        # Get activations and gradients\n",
        "        activations = activation_maps[0][0]  # First batch\n",
        "        grads = gradients[0][0]  # First batch\n",
        "\n",
        "        # Weight activations with gradients (Grad-CAM)\n",
        "        weights = grads.mean(dim=(1, 2, 3), keepdim=True)\n",
        "        cam = (weights * activations).sum(dim=0)\n",
        "\n",
        "        # ReLU\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize\n",
        "        cam = cam.detach().cpu().numpy()\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        # Resize to match input size\n",
        "        cam_resized = zoom(cam, (scan.shape[2] / cam.shape[0],\n",
        "                                 scan.shape[3] / cam.shape[1],\n",
        "                                 scan.shape[4] / cam.shape[2]),\n",
        "                           order=1)\n",
        "\n",
        "        # Get scan as numpy array\n",
        "        scan_np = scan.detach().cpu().numpy()[0, 0]\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "        # Get middle slices\n",
        "        x_mid = scan_np.shape[0] // 2\n",
        "        y_mid = scan_np.shape[1] // 2\n",
        "        z_mid = scan_np.shape[2] // 2\n",
        "\n",
        "        # Plot original images\n",
        "        axes[0, 0].imshow(scan_np[x_mid, :, :], cmap='gray')\n",
        "        axes[0, 0].set_title('Original (Sagittal)')\n",
        "        axes[0, 0].axis('off')\n",
        "\n",
        "        axes[1, 0].imshow(scan_np[:, y_mid, :], cmap='gray')\n",
        "        axes[1, 0].set_title('Original (Coronal)')\n",
        "        axes[1, 0].axis('off')\n",
        "\n",
        "        axes[2, 0].imshow(scan_np[:, :, z_mid], cmap='gray')\n",
        "        axes[2, 0].set_title('Original (Axial)')\n",
        "        axes[2, 0].axis('off')\n",
        "\n",
        "        # Plot Grad-CAM\n",
        "        axes[0, 1].imshow(cam_resized[x_mid, :, :], cmap='hot')\n",
        "        axes[0, 1].set_title('Grad-CAM (Sagittal)')\n",
        "        axes[0, 1].axis('off')\n",
        "\n",
        "        axes[1, 1].imshow(cam_resized[:, y_mid, :], cmap='hot')\n",
        "        axes[1, 1].set_title('Grad-CAM (Coronal)')\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        axes[2, 1].imshow(cam_resized[:, :, z_mid], cmap='hot')\n",
        "        axes[2, 1].set_title('Grad-CAM (Axial)')\n",
        "        axes[2, 1].axis('off')\n",
        "\n",
        "        # Plot overlay\n",
        "        axes[0, 2].imshow(scan_np[x_mid, :, :], cmap='gray')\n",
        "        axes[0, 2].imshow(cam_resized[x_mid, :, :], cmap='hot', alpha=0.5)\n",
        "        axes[0, 2].set_title('Overlay (Sagittal)')\n",
        "        axes[0, 2].axis('off')\n",
        "\n",
        "        axes[1, 2].imshow(scan_np[:, y_mid, :], cmap='gray')\n",
        "        axes[1, 2].imshow(cam_resized[:, y_mid, :], cmap='hot', alpha=0.5)\n",
        "        axes[1, 2].set_title('Overlay (Coronal)')\n",
        "        axes[1, 2].axis('off')\n",
        "\n",
        "        axes[2, 2].imshow(scan_np[:, :, z_mid], cmap='gray')\n",
        "        axes[2, 2].imshow(cam_resized[:, :, z_mid], cmap='hot', alpha=0.5)\n",
        "        axes[2, 2].set_title('Overlay (Axial)')\n",
        "        axes[2, 2].axis('off')\n",
        "\n",
        "        # Set title\n",
        "        pred_class = self.class_names[target_class]\n",
        "        prob = probabilities[0][target_class].item()\n",
        "        age_str = f\", Age: {age.item()}\" if age is not None else \"\"\n",
        "        plt.suptitle(f\"Grad-CAM for class: {pred_class} (prob: {prob:.2f}){age_str}\")\n",
        "\n",
        "        # Save figure\n",
        "        if save_path:\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path, dpi=300)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'gradcam': cam_resized,\n",
        "            'class': pred_class,\n",
        "            'probability': prob\n",
        "        }\n",
        "\n",
        "    def run_inference_pipeline(self, loader, num_samples=10, output_dir=None):\n",
        "        \"\"\"\n",
        "        Run inference pipeline on a set of samples.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        loader : DataLoader\n",
        "            DataLoader to use for inference\n",
        "        num_samples : int\n",
        "            Number of samples to process\n",
        "        output_dir : str or None\n",
        "            Directory to save results\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of prediction results\n",
        "        \"\"\"\n",
        "        if output_dir is None:\n",
        "            output_dir = os.path.join(self.output_dir, \"inference\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Get samples\n",
        "        samples = []\n",
        "        for batch in loader:\n",
        "            # Get the data\n",
        "            scans = batch['scan']\n",
        "            labels = batch['label'].numpy()\n",
        "            patient_ids = batch['patient_id']\n",
        "\n",
        "            # Get age if available\n",
        "            ages = batch.get('age')\n",
        "\n",
        "            # Add samples\n",
        "            for i in range(len(scans)):\n",
        "                samples.append({\n",
        "                    'scan': scans[i].clone(),\n",
        "                    'label': labels[i],\n",
        "                    'label_name': self.class_names[labels[i]],\n",
        "                    'patient_id': patient_ids[i],\n",
        "                    'age': ages[i].item() if ages is not None else None\n",
        "                })\n",
        "\n",
        "                if len(samples) >= num_samples:\n",
        "                    break\n",
        "\n",
        "            if len(samples) >= num_samples:\n",
        "                break\n",
        "\n",
        "        # Run inference on samples\n",
        "        results = []\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            # Visualize prediction\n",
        "            scan = sample['scan']\n",
        "            age = sample['age']\n",
        "            true_label = sample['label_name']\n",
        "            patient_id = sample['patient_id']\n",
        "\n",
        "            # Make prediction\n",
        "            pred_save_path = os.path.join(output_dir, f\"pred_{i}_{patient_id}.png\")\n",
        "            prediction = self.visualize_prediction(scan, age, save_path=pred_save_path)\n",
        "\n",
        "            # Generate Grad-CAM\n",
        "            gradcam_save_path = os.path.join(output_dir, f\"gradcam_{i}_{patient_id}.png\")\n",
        "            gradcam = self.generate_gradcam(scan, target_class=None, age=age, save_path=gradcam_save_path)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'sample_id': i,\n",
        "                'patient_id': patient_id,\n",
        "                'true_label': true_label,\n",
        "                'predicted_label': prediction['class_name'],\n",
        "                'probabilities': prediction['probabilities'],\n",
        "                'age': age,\n",
        "                'correct': true_label == prediction['class_name'],\n",
        "                'prediction_path': pred_save_path,\n",
        "                'gradcam_path': gradcam_save_path\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        # Create summary report\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv(os.path.join(output_dir, \"inference_summary.csv\"), index=False)\n",
        "\n",
        "        # Create summary plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(x='true_label', hue='predicted_label', data=df)\n",
        "        plt.title('True vs Predicted Labels')\n",
        "        plt.xlabel('True Label')\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"label_distribution.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = df['correct'].mean() * 100\n",
        "        print(f\"Inference accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def analyze_age_effects(self, loader, num_age_bins=5, min_samples=10):\n",
        "        \"\"\"\n",
        "        Analyze the effect of age on model predictions.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        loader : DataLoader\n",
        "            DataLoader to use for analysis\n",
        "        num_age_bins : int\n",
        "            Number of age bins to create\n",
        "        min_samples : int\n",
        "            Minimum number of samples per age bin\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Age analysis results\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir = os.path.join(self.output_dir, \"age_analysis\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Collect samples with age information\n",
        "        samples = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(loader, desc=\"Collecting samples\"):\n",
        "                # Get the data\n",
        "                scans = batch['scan']\n",
        "                labels = batch['label'].numpy()\n",
        "\n",
        "                # Skip if age not available\n",
        "                if 'age' not in batch:\n",
        "                    continue\n",
        "\n",
        "                ages = batch['age'].numpy()\n",
        "\n",
        "                # Get predictions\n",
        "                outputs = self.model(scans.to(self.device), batch['age'].to(self.device))\n",
        "                _, predictions = outputs.max(1)\n",
        "                predictions = predictions.cpu().numpy()\n",
        "\n",
        "                # Add samples\n",
        "                for i in range(len(scans)):\n",
        "                    samples.append({\n",
        "                        'age': ages[i],\n",
        "                        'true_label': labels[i],\n",
        "                        'predicted_label': predictions[i],\n",
        "                        'correct': labels[i] == predictions[i]\n",
        "                    })\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(samples)\n",
        "\n",
        "        # Return if not enough samples with age information\n",
        "        if len(df) < min_samples:\n",
        "            print(f\"Not enough samples with age information ({len(df)} < {min_samples})\")\n",
        "            return None\n",
        "\n",
        "        # Create age bins\n",
        "        df['age_bin'] = pd.cut(df['age'], bins=num_age_bins)\n",
        "\n",
        "        # Calculate accuracy per age bin\n",
        "        age_accuracy = df.groupby('age_bin')['correct'].mean() * 100\n",
        "        age_counts = df.groupby('age_bin').size()\n",
        "\n",
        "        # Plot accuracy vs age\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        ax = age_accuracy.plot(kind='bar', color='skyblue')\n",
        "        plt.title('Classification Accuracy by Age Group')\n",
        "        plt.xlabel('Age Group')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Add counts above bars\n",
        "        for i, (acc, count) in enumerate(zip(age_accuracy, age_counts)):\n",
        "            plt.text(i, acc + 1, f\"n={count}\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"accuracy_by_age.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # Analyze confusion patterns by age\n",
        "        age_confusion = {}\n",
        "\n",
        "        for age_bin in df['age_bin'].unique():\n",
        "            bin_df = df[df['age_bin'] == age_bin]\n",
        "\n",
        "            if len(bin_df) < 5:  # Skip bins with too few samples\n",
        "                continue\n",
        "\n",
        "            cm = confusion_matrix(\n",
        "                bin_df['true_label'],\n",
        "                bin_df['predicted_label'],\n",
        "                labels=range(len(self.class_names))\n",
        "            )\n",
        "\n",
        "            # Normalize by row\n",
        "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "            age_confusion[age_bin] = {\n",
        "                'confusion_matrix': cm,\n",
        "                'normalized_confusion_matrix': cm_norm,\n",
        "                'sample_count': len(bin_df)\n",
        "            }\n",
        "\n",
        "            # Plot confusion matrix\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(\n",
        "                cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=self.class_names,\n",
        "                yticklabels=self.class_names\n",
        "            )\n",
        "            plt.title(f'Confusion Matrix for Age Group {age_bin} (n={len(bin_df)})')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, f\"cm_age_{age_bin.left:.0f}_{age_bin.right:.0f}.png\"), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "        # Save data\n",
        "        df.to_csv(os.path.join(output_dir, \"age_analysis.csv\"), index=False)\n",
        "\n",
        "        return {\n",
        "            'age_accuracy': age_accuracy,\n",
        "            'age_counts': age_counts,\n",
        "            'age_confusion': age_confusion,\n",
        "            'samples': df\n",
        "        }"
      ],
      "metadata": {
        "id": "ZWOP5eCyKoJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pipeline(model_path=None, data_dir=DATA_DIR, output_dir='results',\n",
        "                   batch_size=4, num_workers=4, device=None, widening_factor=8, use_age=True):\n",
        "    \"\"\"\n",
        "    Create a pipeline for testing and visualizing the Alzheimer's CNN model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_path : str or None\n",
        "        Path to the model checkpoint file\n",
        "    data_dir : str\n",
        "        Directory containing the processed data files\n",
        "    output_dir : str\n",
        "        Directory to save results\n",
        "    batch_size : int\n",
        "        Batch size for the data loaders\n",
        "    num_workers : int\n",
        "        Number of workers for the data loaders\n",
        "    device : torch.device or None\n",
        "        Device to run the model on (CPU or GPU)\n",
        "    widening_factor : int\n",
        "        Widening factor for the model\n",
        "    use_age : bool\n",
        "        Whether to use age information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    AlzheimerPipeline\n",
        "        Pipeline object\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(\n",
        "        data_dir=data_dir,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = AlzheimerCNN(widening_factor=widening_factor, use_age=use_age)\n",
        "\n",
        "    # Load model if path provided\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"Loaded model from {model_path} (epoch {checkpoint.get('epoch', 'unknown')})\")\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = AlzheimerPipeline(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "txjjDSsDeExE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'results'\n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "widening_factor = 8\n",
        "use_age = True\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = create_pipeline(\n",
        "    data_dir=DATA_DIR,\n",
        "    output_dir=output_dir,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    widening_factor=widening_factor,\n",
        "    use_age=use_age\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "pipeline.train(num_epochs=num_epochs, lr=learning_rate, weight_decay=1e-4, patience=10)\n",
        "\n",
        "# Test the model\n",
        "pipeline.test()\n",
        "\n",
        "# Visualize saliency maps\n",
        "pipeline.visualize_saliency_maps(num_samples=3)\n",
        "\n",
        "# Run inference pipeline\n",
        "pipeline.run_inference_pipeline(pipeline.test_loader, num_samples=10)\n",
        "\n",
        "# Analyze age effects\n",
        "pipeline.analyze_age_effects(pipeline.test_loader, num_age_bins=5)\n",
        "\n",
        "# Export metrics\n",
        "pipeline.export_metrics()\n",
        "\n",
        "print(\"Pipeline completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELynjC42eITz",
        "outputId": "e2c45ef1-08b6-4e21-d5a9-1d69d7bac425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 900 samples from 3 zip files\n",
            "Created data loaders with 630 training, 135 validation, and 135 test samples\n",
            "Feature size after convolutions: 512\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]:   3%|▎         | 5/158 [05:40<2:48:52, 66.23s/it, acc=40.00%, loss=1.5785]"
          ]
        }
      ]
    }
  ]
}